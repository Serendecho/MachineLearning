{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分析与预处理\n",
    "在开始进入深度学习模型相关内容之前，我们往往需要对表达不规范的文本内容进行清洗，这里我们对文本的一般处理步骤进行了梳理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 英文文本分析与预处理\n",
    "### 1.1 Tokenization（标记化/ 分词）\n",
    "文本是不能成段送入模型中进行分析的，我们通常会把文本切成有独立含义的字、词或者短语，这个过程叫做tokenization，这通常是大家解决自然语言处理问题的第一步。在NLTK中提供了2种不同方式的tokenization：\n",
    "（1）sentence tokenization：把文本进行“断句” 和 （2）word tokenization：对文本进行“分词”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   # 用于英文文本预处理\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "test_sents = \"This is a test sentence. This sentence is used for testing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a test sentence.', 'This sentence is used for testing!']\n"
     ]
    }
   ],
   "source": [
    "# 断句（进行句子与句子之间的拆分）\n",
    "sentences = sent_tokenize(test_sents)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "words = word_tokenize(sentences[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 2 去停用词\n",
    "在自然语言处理的很多任务中，我们处理的主体“文本”中有一些功能词经常出现，然而对于最后的任务目标并没有帮助，甚至会对统计方法带来一些干扰，我们把这类词叫做停用词，通常我们会用一个停用词表把它们过滤出来。比如英语当中的定冠词/不定冠词(a,an,the等)。\n",
    "\n",
    "在nltk的语料库nltk.corpus中，包含了英文停用词的列表，可以通过停用词列表将原始语料进行清洗，从而保留跟任务强相关的文本内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入内置停用词（nltk的是针对英文文本的）\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'test', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "# 去掉停用词\n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 词性标注\n",
    "词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。\n",
    "\n",
    "词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。\n",
    "\n",
    "词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。在一些特定任务中，通过给词赋予词性能够为文本分析添加更多的信息，从而进一步提高任务的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|tag|mean|释义|例子|\n",
    "|:-|:-|:-|:-|\n",
    "|CC|Coordinating conjunction|连词|and, or,but, if, while,although|\n",
    "|CD|Cardinal number|数词|twenty-four, fourth, 1991,14:24|\n",
    "|DT|Determiner|限定词|the, a, some, most,every, no|\n",
    "|EX|Existential there|存在量词|there, there’s|\n",
    "|FW|Foreign word|外来词|dolce, ersatz, esprit, quo,maitre|\n",
    "|IN|Preposition or subordinating conjunction|介词连词|on, of,at, with,by,into, under|\n",
    "|JJ|Adjective|形容词|new,good, high, special, big, local|\n",
    "|JJR|Adjective, comparative|比较级词语|bleaker braver breezier briefer brighter brisker|\n",
    "|JJS|Adjective, superlative|最高级词语|calmest cheapest choicest classiest cleanest clearest|\n",
    "|LS|List item marker|标记|A A. B B. C C. D E F First G H I J K|\n",
    "|MD|Modal|情态动词|can cannot could couldn’t|\n",
    "|NN|Noun singular or mass|名词|year,home, costs, time, education|\n",
    "|NNS|Noun, plural|名词复数|undergraduates scotches|\n",
    "|NNP|Proper noun, singular|专有名词|Alison,Africa,April,Washington|\n",
    "|NNPS|Proper noun, plural|专有名词复数|Americans Americas Amharas Amityvilles|\n",
    "|PDT|Predeterminer|前限定词|all both half many|\n",
    "|POS|Possessive ending|所有格标记|’ ‘s|\n",
    "|PRP|Personal pronoun|人称代词|hers herself him himself hisself|\n",
    "|PRP\\$ |Possessive pronoun|所有格|her his mine my our ours|\n",
    "|RB|Adverb|副词|occasionally unabatingly maddeningly|\n",
    "|RBR|Adverb, comparative|副词比较级|further gloomier grander|\n",
    "|RBS|Adverb, superlative|副词最高级|best biggest bluntest earliest|\n",
    "|RP|Particle|虚词|aboard about across along apart|\n",
    "|SYM|Symbol|符号|% & ’ ” ”. ) )|\n",
    "|TO|to|词to|to|\n",
    "|UH|Interjection|感叹词|Goodbye Goody Gosh Wow|\n",
    "|VB|Verb, base form|动词|ask assemble assess|\n",
    "|VBD|Verb, past tense|动词过去式|dipped pleaded swiped|\n",
    "|VBG|Verb, gerund or present participle|动词现在分词|telegraphing stirring focusing|\n",
    "|VBN|Verb, past participle|动词过去分词|multihulled dilapidated aerosolized|\n",
    "|VBP|Verb, non-3rd person singular present|动词现在式非第三人称时态|predominate wrap resort sue|\n",
    "|VBZ|Verb, 3rd person singular present|动词现在式第三人称时态|bases reconstructs marks|\n",
    "|WDT|Wh-determiner|Wh限定词|who,which,when,what,where,how|\n",
    "|WP|Wh-pronoun|WH代词|that what whatever|\n",
    "|WP\\$|Possessive wh-pronoun|WH代词所有格|whose|\n",
    "|WRB|Wh-adverb|WH副词| |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择标签并进行标注\n",
    "tags = set(['MD', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "           'RP', 'RB', 'RBR', 'RBS', 'JJ', 'JJR', 'JJS'])\n",
    "filtered_sentence_pos_tags = nltk.pos_tag(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('test', 'NN'), ('sentence', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence_pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stemming and Lemmatizing\n",
    "很多时候我们需要对英文当中的时态语态等做归一化，这个时候我们就需要stemming和lemmatizing这样的操作了。比如\"running\"是\"run\"的进行时，但是这个词表征的含义和\"run\"是一致的，当我们在分析语义信息时，我们希望能够将这两个词的实际含义信息进行对齐。\n",
    "\n",
    "其中Lemmatization和Stemmer很类似，不同的地方在于它还考虑了词义关联等信息，Stemmer的速度更快，但是它通常只是一系列的规则，各位同学可以依据自己的实际情况采用对应的方法\n",
    "\n",
    "调用方法包括nltk.stem中的 PorterStemmer以及WordNetLemmatizer 两种方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 中文文本分析与预处理\n",
    "中文文本分析不同于英文文本分析，词与词之间并没有通过空格来表征界限。而且中文也没有如英文时态表达相似的模型，因此在进行处理时需要采用其特有的模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 分词\n",
    "对于中文和日文这样的特殊亚洲语系文本而言，字和字之间是紧密相连的，单纯从文本形态上无法区分具备独立含义的词（拉丁语系纯天然由空格分隔不同的word），而不同的词以不同的方式排布，可以表达不同的内容和情感，因此在很多中文任务中，我们需要做的第一个处理叫做分词。分词的差异将会为上游任务带来完全不同的效果。 \n",
    "\n",
    "主流的分词工具库包括 中科院计算所NLPIR、哈工大LTP、清华大学THULAC、Hanlp分词器、Python jieba工具库等。\n",
    "\n",
    "分词工具一般都支持添加自己自带的特殊词词典，如jieba可以采用jieba.load_userdict(file_name)加载用户字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1.WIN\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.144 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/ 在/ 学习/ 自然语言/ 处理\n"
     ]
    }
   ],
   "source": [
    "# encoding = utf-8\n",
    "import jieba\n",
    "\n",
    "# 使用精确模式进行分词\n",
    "seg_list = jieba.cut(\"我在学习自然语言处理\", cut_all = False)\n",
    "# 将分好的词用/ 隔开\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/ 在/ 学习/ 自然语言处理\n"
     ]
    }
   ],
   "source": [
    "# 传入用户自定义的固定表达到用户字典\n",
    "jieba.load_userdict(['自然语言处理'])\n",
    "\n",
    "# 再次使用精确模式进行分词\n",
    "seg_list = jieba.cut(\"我在学习自然语言处理\", cut_all = False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 去停用词\n",
    "中文当中常用到的停用词词表可以参见[中文常用停用词表](https://github.com/goto456/stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 中文进阶分析\n",
    "中文进阶分析包括了词性分析、命名实体识别、依存句法分析、语义角色标注、语义依存分析等。可以采用jieba进行处理，也可以使用LTP等工具进行处理。\n",
    "\n",
    "LTP依赖于pytorch进行构建，说明文档参照[LTP使用说明](http://ltp.ai/docs/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
