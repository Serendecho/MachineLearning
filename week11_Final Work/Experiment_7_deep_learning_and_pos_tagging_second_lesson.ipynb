{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "train_data = './data/ctb5.1-pos/train.tsv'\n",
    "test_data = './data/ctb5.1-pos/test.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t', skip_blank_lines=False, header=None)\n",
    "    # 取出文本部分\n",
    "    content = data[0]\n",
    "    # 取出标签部分\n",
    "    label = data[1]\n",
    "    \n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/ctb5.1-pos/train.tsv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-09d112acbae9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 读取数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-6372be196f50>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_blank_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 取出文本部分\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# 取出标签部分\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3-5.3.0\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3-5.3.0\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3-5.3.0\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3-5.3.0\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3-5.3.0\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'./data/ctb5.1-pos/train.tsv' does not exist"
     ]
    }
   ],
   "source": [
    "# 读取数据集\n",
    "X_train, y_train = get_data(train_data)\n",
    "X_test, y_test = get_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理与格式转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 'LC', 'FW', 'PN', 'VV', 'X', 'IJ', 'P', 'SB', 'OD', 'MSP', 'VP', 'CC', 'PU', 'DEC', 'NR', 'ETC', 'VC', 'CD', 'DT', 'DER', 'AD', 'VE', 'CS', 'BA', 'AS', 'DEG', 'NT', 'SP', 'VA', 'DEV', 'JJ', 'NN', 'LB', 'NP', 'M']\n",
      "520125 36 {nan: 18426, 'LC': 7782, 'FW': 33, 'PN': 6644, 'VV': 69858, 'X': 6, 'IJ': 12, 'P': 17606, 'SB': 455, 'OD': 1675, 'MSP': 1336, 'VP': 1, 'CC': 7355, 'PU': 76753, 'DEC': 12510, 'NR': 30570, 'ETC': 1303, 'VC': 5404, 'CD': 16182, 'DT': 5986, 'DER': 258, 'AD': 36430, 'VE': 3005, 'CS': 892, 'BA': 755, 'AS': 4118, 'DEG': 12337, 'NT': 9659, 'SP': 468, 'VA': 7755, 'DEV': 634, 'JJ': 13234, 'NN': 136643, 'LB': 245, 'NP': 5, 'M': 13790}\n"
     ]
    }
   ],
   "source": [
    "# 构建标签字典\n",
    "labels = y_train.tolist() + y_test.tolist()\n",
    "labels_types = list(set(labels))\n",
    "print(labels_types)\n",
    "\n",
    "labels_dict = {}\n",
    "labels_index = {\"padded_label\" : 0}\n",
    "\n",
    "for index in range(len(labels_types)):\n",
    "    label = labels_types[index]\n",
    "    labels_dict.update({label: labels.count(label)})\n",
    "    labels_index.update({label: index+1})\n",
    "\n",
    "np.save('y_labels_index.npy', labels_index) \n",
    "print(len(labels), len(labels_dict), labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'padded_label': 0, nan: 1, 'LC': 2, 'FW': 3, 'PN': 4, 'VV': 5, 'X': 6, 'IJ': 7, 'P': 8, 'SB': 9, 'OD': 10, 'MSP': 11, 'VP': 12, 'CC': 13, 'PU': 14, 'DEC': 15, 'NR': 16, 'ETC': 17, 'VC': 18, 'CD': 19, 'DT': 20, 'DER': 21, 'AD': 22, 'VE': 23, 'CS': 24, 'BA': 25, 'AS': 26, 'DEG': 27, 'NT': 28, 'SP': 29, 'VA': 30, 'DEV': 31, 'JJ': 32, 'NN': 33, 'LB': 34, 'NP': 35, 'M': 36}\n"
     ]
    }
   ],
   "source": [
    "# 构建的Y标签字典\n",
    "print(labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以句子进行拆分后的句子为：\n",
      " [array(['上海', '浦东', '开发', '与', '法制', '建设', '同步'], dtype=object), array(['新华社', '上海', '二月', '十日', '电', '（', '记者', '谢金虎', '、', '张持坚', '）'],\n",
      "      dtype=object), array(['上海', '浦东', '近年', '来', '颁布', '实行', '了', '涉及', '经济', '、', '贸易', '、',\n",
      "       '建设', '、', '规划', '、', '科技', '、', '文教', '等', '领域', '的', '七十一', '件',\n",
      "       '法规性', '文件', '，', '确保', '了', '浦东', '开发', '的', '有序', '进行', '。'],\n",
      "      dtype=object), array(['浦东', '开发', '开放', '是', '一', '项', '振兴', '上海', '，', '建设', '现代化',\n",
      "       '经济', '、', '贸易', '、', '金融', '中心', '的', '跨世纪', '工程', '，', '因此',\n",
      "       '大量', '出现', '的', '是', '以前', '不', '曾', '遇到', '过', '的', '新', '情况',\n",
      "       '、', '新', '问题', '。'], dtype=object), array(['对', '此', '，', '浦东', '不', '是', '简单', '的', '采取', '“', '干', '一', '段',\n",
      "       '时间', '，', '等', '积累', '了', '经验', '以后', '再', '制定', '法规', '条例', '”',\n",
      "       '的', '做法', '，', '而', '是', '借鉴', '发达', '国家', '和', '深圳', '等', '特区',\n",
      "       '的', '经验', '教训', '，', '聘请', '国内外', '有关', '专家', '学者', '，', '积极',\n",
      "       '、', '及时', '地', '制定', '和', '推出', '法规性', '文件', '，', '使', '这些', '经济',\n",
      "       '活动', '一', '出现', '就', '被', '纳入', '法制', '轨道', '。'], dtype=object)]\n",
      "以句子进行拆分后的句子所对应的词性为：\n",
      " [array(['NR', 'NR', 'NN', 'CC', 'NN', 'NN', 'VV'], dtype=object), array(['NN', 'NR', 'NT', 'NT', 'NN', 'PU', 'NN', 'NR', 'PU', 'NR', 'PU'],\n",
      "      dtype=object), array(['NR', 'NR', 'NT', 'LC', 'VV', 'VV', 'AS', 'VV', 'NN', 'PU', 'NN',\n",
      "       'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'ETC', 'NN', 'DEC',\n",
      "       'CD', 'M', 'NN', 'NN', 'PU', 'VV', 'AS', 'NR', 'NN', 'DEG', 'JJ',\n",
      "       'NN', 'PU'], dtype=object), array(['NR', 'NN', 'NN', 'VC', 'CD', 'M', 'VV', 'NR', 'PU', 'VV', 'NN',\n",
      "       'NN', 'PU', 'NN', 'PU', 'NN', 'NN', 'DEC', 'JJ', 'NN', 'PU', 'AD',\n",
      "       'AD', 'VV', 'DEC', 'VC', 'NT', 'AD', 'AD', 'VV', 'AS', 'DEC', 'JJ',\n",
      "       'NN', 'PU', 'JJ', 'NN', 'PU'], dtype=object), array(['P', 'PN', 'PU', 'NR', 'AD', 'VC', 'VA', 'DEV', 'VV', 'PU', 'VV',\n",
      "       'CD', 'M', 'NN', 'PU', 'P', 'VV', 'AS', 'NN', 'LC', 'AD', 'VV',\n",
      "       'NN', 'NN', 'PU', 'DEC', 'NN', 'PU', 'CC', 'VC', 'VV', 'JJ', 'NN',\n",
      "       'CC', 'NR', 'ETC', 'NN', 'DEG', 'NN', 'NN', 'PU', 'VV', 'NN', 'JJ',\n",
      "       'NN', 'NN', 'PU', 'AD', 'PU', 'AD', 'DEV', 'VV', 'CC', 'VV', 'NN',\n",
      "       'NN', 'PU', 'VV', 'DT', 'NN', 'NN', 'AD', 'VV', 'AD', 'SB', 'VV',\n",
      "       'NN', 'NN', 'PU'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# 按句对X、y进行拆分\n",
    "def split_corpus_by_sentence(content):\n",
    "    cleaned_sentence = []\n",
    "    split_label = content.isnull()\n",
    "    last_split_index = 0\n",
    "    index = 0\n",
    "    while index < len(content):\n",
    "        current_word = content[index]\n",
    "        if split_label[index] == True and len(cleaned_sentence) == 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index:index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "        elif split_label[index] == True  and len(cleaned_sentence) > 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index:index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "        else:\n",
    "            index += 1\n",
    "    return cleaned_sentence\n",
    "\n",
    "X_train_sent_split = split_corpus_by_sentence(X_train)\n",
    "y_train_sent_split = split_corpus_by_sentence(y_train)\n",
    "X_test_sent_split = split_corpus_by_sentence(X_test)\n",
    "y_test_sent_split = split_corpus_by_sentence(y_test)\n",
    "\n",
    "print('以句子进行拆分后的句子为：\\n', X_train_sent_split[:5])\n",
    "print('以句子进行拆分后的句子所对应的词性为：\\n', y_train_sent_split[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15, 15, 32, 12, 32, 32, 4], [32, 15, 27, 27, 32, 13, 32, 15, 13, 15, 13], [15, 15, 27, 1, 4, 4, 25, 4, 32, 13, 32, 13, 32, 13, 32, 13, 32, 13, 32, 16, 32, 14, 18, 35, 32, 32, 13, 4, 25, 15, 32, 26, 31, 32, 13], [15, 32, 32, 17, 18, 35, 4, 15, 13, 4, 32, 32, 13, 32, 13, 32, 32, 14, 31, 32, 13, 21, 21, 4, 14, 17, 27, 21, 21, 4, 25, 14, 31, 32, 13, 31, 32, 13], [7, 3, 13, 15, 21, 17, 29, 30, 4, 13, 4, 18, 35, 32, 13, 7, 4, 25, 32, 1, 21, 4, 32, 32, 13, 14, 32, 13, 12, 17, 4, 31, 32, 12, 15, 16, 32, 26, 32, 32, 13, 4, 32, 31, 32, 32, 13, 21, 13, 21, 30, 4, 12, 4, 32, 32, 13, 4, 19, 32, 32, 21, 4, 21, 8, 4, 32, 32, 13]]\n"
     ]
    }
   ],
   "source": [
    "def transfer_label_category_index(origin_labels, labels_types):\n",
    "    transfered_label = []\n",
    "    for sentence_labels in origin_labels:\n",
    "        labels_format_index = [labels_types.index(label) for label in sentence_labels]  # 将标签依据字典转化为序号\n",
    "        transfered_label.append(labels_format_index)\n",
    "    return transfered_label\n",
    "\n",
    "y_train_index = transfer_label_category_index(y_train_sent_split, labels_types)\n",
    "y_test_index = transfer_label_category_index(y_test_sent_split, labels_types)\n",
    "\n",
    "print(y_train_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# 标签格式转化\n",
    "# 构建对应（标签样本数，句子长度，标签类别数）形状的张量，值全为0\n",
    "y_train_index_padded = np.zeros((len(y_train_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), dtype='float', order='C')\n",
    "y_test_index_padded = np.zeros((len(y_test_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), dtype='float', order='C')\n",
    "\n",
    "# 填充张量\n",
    "for sentence_labels_index in range(len(y_train_index)):\n",
    "    for label_index in range(len(y_train_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_train_index_padded[sentence_labels_index, label_index, y_train_index[sentence_labels_index][label_index]+1] = 1\n",
    "    \n",
    "    if len(y_train_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_train_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_train_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "\n",
    "# 优化：若为填充的标签，则将其预测为第一位为1\n",
    "\n",
    "for sentence_labels_index in range(len(y_test_index)):\n",
    "    for label_index in range(len(y_test_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_test_index_padded[sentence_labels_index, label_index, y_test_index[sentence_labels_index][label_index]+1] = 1\n",
    "    \n",
    "    if len(y_test_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_test_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_test_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "\n",
    "print(y_train_index_padded[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec模型导入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练的word2vec模型采用前人用中文维基百科训练好的模型，请各位同学进入链接下载，并放到相对本脚本 同一级文件夹data 的目录下 并解压。\n",
    "\n",
    "[word2vec模型链接](https://github.com/Embedding/Chinese-Word-Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "## 1 导入 预训练的词向量\n",
    "myPath = './data/sgns.wiki.word' # 本地词向量的地址\n",
    "Word2VecModel = gensim.models.KeyedVectors.load_word2vec_format(myPath).wv # 读取词向量，以二进制读取\n",
    "\n",
    "vector = Word2VecModel.wv['空间']  # 词语的向量，是numpy格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "，\n",
      "Vocab(count:352217, index:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(type(Word2VecModel.wv)) # 结果为：Word2VecKeyedVectors\n",
    "\n",
    "for i,j in Word2VecModel.wv.vocab.items():\n",
    "    print(i) # 此时 i 代表每个单词\n",
    "    print(j) # j 代表封装了 词频 等信息的 gensim“Vocab”对象，例子：Vocab(count:1481, index:38, sample_int:3701260191)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## 2 构造包含所有词语的 list，以及初始化 “词语-序号”字典 和 “词向量”矩阵\n",
    "vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]# 存储 所有的 词语\n",
    "\n",
    "word_index = {\" \": 0}# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "word_vector = {} # 初始化`[word : vector]`字典\n",
    "\n",
    "# 初始化存储所有向量的大矩阵，留意其中多一位（首行），词向量全为 0，用于 padding补零。\n",
    "# 行数 为 所有单词数+1 比如 10000+1 ； 列数为 词向量“维度”比如100。\n",
    "embeddings_matrix = np.zeros((len(vocab_list) + 1, Word2VecModel.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "## 3 填充 上述 的字典 和 大矩阵\n",
    "for i in range(len(vocab_list)):\n",
    "    # print(i)\n",
    "    word = vocab_list[i]  # 每个词语\n",
    "    word_index[word] = i + 1 # 词语：序号\n",
    "    word_vector[word] = Word2VecModel.wv[word] # 词语：词向量\n",
    "    embeddings_matrix[i + 1] = Word2VecModel.wv[word]  # 词向量矩阵\n",
    "\n",
    "np.save('x_word_index.npy', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  347 16980   507    10 15537   603  4380     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "# 序号化 文本，tokenizer句子，并返回每个句子所对应的词语索引\n",
    "\n",
    "# 由于将词语转化为索引的word_index需要与词向量模型对齐，故在导入词向量模型后再将X进行处理\n",
    "def tokenizer(texts, word_index):\n",
    "    data = []\n",
    "    for sentence in texts:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                new_sentence.append(word_index[word])  # 把文本中的 词语转化为index\n",
    "            except:\n",
    "                new_sentence.append(0)\n",
    "            \n",
    "        data.append(new_sentence)\n",
    "    # 使用kears的内置函数padding对齐句子,好处是输出numpy数组，不用自己转化了\n",
    "    data = sequence.pad_sequences(data, maxlen = MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    return data\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train_sent_split, word_index)\n",
    "X_test_tokenized = tokenizer(X_test_sent_split, word_index)\n",
    "\n",
    "print(X_train_tokenized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标引网络构建及训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import keras\n",
    "from keras import optimizers\n",
    "\n",
    "EMBEDDING_DIM = 300 #词向量维度\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） \n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                   ))\n",
    "# input shape (Batch_size, Time_step, Input_Sizes)\n",
    "model.add(LSTM(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, input_shape=(MAX_SEQUENCE_LENGTH, 128), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels_types)+1, input_shape=(MAX_SEQUENCE_LENGTH, 64), activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          105665400 \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 64)           8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100, 37)           2405      \n",
      "=================================================================\n",
      "Total params: 105,895,709\n",
      "Trainable params: 230,309\n",
      "Non-trainable params: 105,665,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18078, 100) (18078, 100, 37)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tokenized.shape, y_train_index_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "18078/18078 [==============================] - 56s 3ms/step - loss: 0.4040 - accuracy: 0.8862\n",
      "Epoch 2/20\n",
      "18078/18078 [==============================] - 55s 3ms/step - loss: 0.1643 - accuracy: 0.9480\n",
      "Epoch 3/20\n",
      "18078/18078 [==============================] - 62s 3ms/step - loss: 0.1399 - accuracy: 0.9565\n",
      "Epoch 4/20\n",
      "18078/18078 [==============================] - 67s 4ms/step - loss: 0.1277 - accuracy: 0.9602\n",
      "Epoch 5/20\n",
      "18078/18078 [==============================] - 61s 3ms/step - loss: 0.1203 - accuracy: 0.9623\n",
      "Epoch 6/20\n",
      "18078/18078 [==============================] - 58s 3ms/step - loss: 0.1150 - accuracy: 0.9638\n",
      "Epoch 7/20\n",
      "18078/18078 [==============================] - 55s 3ms/step - loss: 0.1107 - accuracy: 0.9650\n",
      "Epoch 8/20\n",
      "18078/18078 [==============================] - 57s 3ms/step - loss: 0.1070 - accuracy: 0.9660\n",
      "Epoch 9/20\n",
      "18078/18078 [==============================] - 57s 3ms/step - loss: 0.1039 - accuracy: 0.9672\n",
      "Epoch 10/20\n",
      "18078/18078 [==============================] - 54s 3ms/step - loss: 0.1016 - accuracy: 0.9678\n",
      "Epoch 11/20\n",
      "18078/18078 [==============================] - 54s 3ms/step - loss: 0.0995 - accuracy: 0.9684\n",
      "Epoch 12/20\n",
      "18078/18078 [==============================] - 59s 3ms/step - loss: 0.0975 - accuracy: 0.9689\n",
      "Epoch 13/20\n",
      "18078/18078 [==============================] - 59s 3ms/step - loss: 0.0961 - accuracy: 0.9694\n",
      "Epoch 14/20\n",
      "18078/18078 [==============================] - 58s 3ms/step - loss: 0.0944 - accuracy: 0.9700\n",
      "Epoch 15/20\n",
      "18078/18078 [==============================] - 58s 3ms/step - loss: 0.0929 - accuracy: 0.9703\n",
      "Epoch 16/20\n",
      "18078/18078 [==============================] - 69s 4ms/step - loss: 0.0911 - accuracy: 0.9709\n",
      "Epoch 17/20\n",
      "18078/18078 [==============================] - 59s 3ms/step - loss: 0.0902 - accuracy: 0.9712\n",
      "Epoch 18/20\n",
      "18078/18078 [==============================] - 53s 3ms/step - loss: 0.0891 - accuracy: 0.9715\n",
      "Epoch 19/20\n",
      "18078/18078 [==============================] - 52s 3ms/step - loss: 0.0879 - accuracy: 0.9720\n",
      "Epoch 20/20\n",
      "18078/18078 [==============================] - 53s 3ms/step - loss: 0.0871 - accuracy: 0.9722\n",
      "evaluation!\n",
      "348/348 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_tokenized, y_train_index_padded,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          verbose=1\n",
    "         )\n",
    "print('evaluation!')\n",
    "score = model.evaluate(X_test_tokenized, y_test_index_padded, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cn_pos_tag.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练的模型经在测试集上验证获得的loss和accuracy为：\n",
      "[0.054519426377340294, 0.982557475566864]\n"
     ]
    }
   ],
   "source": [
    "print('训练的模型经在测试集上验证获得的loss和accuracy为：')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.7863530e-06 1.3260282e-08 5.1007342e-01 ... 3.3138915e-07\n",
      "   5.6417657e-09 7.0415095e-05]\n",
      "  [1.5724202e-08 4.0942895e-13 2.9143064e-06 ... 1.8065069e-11\n",
      "   7.4609210e-14 3.3791670e-07]\n",
      "  [8.4986749e-21 3.6002287e-20 2.4648545e-13 ... 4.1570019e-09\n",
      "   1.7601839e-18 1.6017105e-09]\n",
      "  ...\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test_tokenized[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5.5427216e-16 5.9282714e-28 1.4246143e-22 ... 6.5725498e-34\n",
      "   5.4068325e-31 1.9028390e-18]\n",
      "  [1.3030440e-05 6.9722041e-08 3.1839829e-02 ... 6.0993862e-08\n",
      "   1.3876459e-08 1.2540212e-04]\n",
      "  [5.0733701e-10 1.3419541e-15 6.1689619e-08 ... 1.1741996e-13\n",
      "   2.0201592e-16 2.7416769e-08]\n",
      "  ...\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test_tokenized[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_index_padded[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18078/18078 [==============================] - 24s 1ms/step - loss: 0.4788 - accuracy: 0.8636\n",
      "Epoch 2/2\n",
      "18078/18078 [==============================] - 23s 1ms/step - loss: 0.2138 - accuracy: 0.9327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fcd7af3c4d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import GRU, SimpleRNN\n",
    "\n",
    "model_RNN = Sequential()\n",
    "model_RNN.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） \n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                   ))\n",
    "# input shape (Batch_size, Time_step, Input_Sizes)\n",
    "model_RNN.add(SimpleRNN(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), activation='tanh', return_sequences=True))\n",
    "model_RNN.add(Dropout(0.5))\n",
    "model_RNN.add(Dense(64, input_shape=(MAX_SEQUENCE_LENGTH, 128), activation='relu'))\n",
    "model_RNN.add(Dropout(0.5))\n",
    "model_RNN.add(Dense(len(labels_types)+1, input_shape=(MAX_SEQUENCE_LENGTH, 64), activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "model_RNN.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_RNN.fit(X_train_tokenized, y_train_index_padded,\n",
    "          epochs=2,\n",
    "          batch_size=128,\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18078/18078 [==============================] - 48s 3ms/step - loss: 0.4046 - accuracy: 0.8971\n",
      "Epoch 2/2\n",
      "18078/18078 [==============================] - 49s 3ms/step - loss: 0.1704 - accuracy: 0.9452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fcc12fa6690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GRU = Sequential()\n",
    "model_GRU.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） \n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                   ))\n",
    "# input shape (Batch_size, Time_step, Input_Sizes)\n",
    "model_GRU.add(GRU(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), activation='tanh', return_sequences=True))\n",
    "model_GRU.add(Dropout(0.5))\n",
    "model_GRU.add(Dense(64, input_shape=(MAX_SEQUENCE_LENGTH, 128), activation='relu'))\n",
    "model_GRU.add(Dropout(0.5))\n",
    "model_GRU.add(Dense(len(labels_types)+1, input_shape=(MAX_SEQUENCE_LENGTH, 64), activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "model_GRU.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_GRU.fit(X_train_tokenized, y_train_index_padded,\n",
    "          epochs=2,\n",
    "          batch_size=128,\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
