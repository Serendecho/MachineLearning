{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现用户输入，进行POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相应的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding='UTF-8'\n",
    "###\n",
    "# 将训练好的词性标注模型封装为服务\n",
    "###\n",
    "# 加载存储的模型开始预测\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文本中的词语转换为索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于将词语转化为索引的word_index需要与词向量模型对齐，故在导入词向量模型后再将X进行处理\n",
    "def tokenizer(texts, word_index):\n",
    "    data = []\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "    for sentence in texts:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                new_sentence.append(word_index[word])  # 把文本中的词语转化为index\n",
    "            except:\n",
    "                new_sentence.append(0)\n",
    "            \n",
    "        data.append(new_sentence)\n",
    "    # 使用kears的内置函数padding对齐句子,好处是输出numpy数组，不用自己转化了\n",
    "    data = sequence.pad_sequences(data, maxlen = MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将输入的文本转换为索引列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_input(input_text_segged):\n",
    "    # 序号化 文本，tokenizer句子，并返回每个句子所对应的词语索引，并填充句子长度至100\n",
    "    # 因为只有一句，所以在该输入句外加上了括号\n",
    "    text_dictionary = np.load('./word_index.npy', allow_pickle=True).item()  # 导入{索引：词语}字典\n",
    "    padded_input_text_indexs = tokenizer([input_text_segged], text_dictionary)\n",
    "    \n",
    "    return padded_input_text_indexs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将输出转化为标签格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_output(output):\n",
    "    # 导入{标签：索引}字典\n",
    "    label_dictionary = np.load('./labels_index.npy', allow_pickle=True).item()\n",
    "    label_dictionary_index_label = {label_dictionary[key]:key for key in label_dictionary}\n",
    "    \n",
    "    # 将预测的每一个标签对应向量转化为索引号\n",
    "    output_index = []\n",
    "    for label_vector in output[0]:\n",
    "        label_max_prob = max(label_vector.tolist())\n",
    "        label_max_prob_index = label_vector.tolist().index(label_max_prob)\n",
    "        output_index.append(label_max_prob_index)\n",
    "    processed_output = [label_dictionary_index_label[index] for index in output_index]\n",
    "\n",
    "    return processed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将输入的文本进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_text_seg(text_to_be_seg):\n",
    "    # 将输入的文本进行分词处理\n",
    "    print(\">>> Processing input text...\")\n",
    "    segged_text = jieba.cut(text_to_be_tag, cut_all=False)\n",
    "    segged_text = '/'.join(segged_text)\n",
    "    segged_text = segged_text.split('/')\n",
    "    print('>>> Segged text：  ', segged_text)\n",
    "\n",
    "    return segged_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对输入的文本进行POS预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_text_predict(segged_text):\n",
    "    # 将分词后的输入文本对应模型训练时所用的词典，转化为index\n",
    "    processed_input = transfer_input(segged_text)\n",
    "    \n",
    "    # 模型预测\n",
    "    print(\">>> Using loaded model to predict...\")\n",
    "    output = pos_model.predict(processed_input)\n",
    "    \n",
    "    # 将输出转化为标签格式\n",
    "    processed_output = transfer_output(output)\n",
    "    \n",
    "    # 将原词和词性组合输出\n",
    "    final_output = []\n",
    "    for index in range(len(segged_text)):\n",
    "        final_output.append({segged_text[index]:processed_output[index]})\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取用户输入，加载模型进行预测，输出预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = = = = = = = = = = = = = = = =  Loading model... = = = = = = = = = = = = = = = = = = = = \n",
      "\n",
      "Model loaded successfully!\n",
      "\n",
      " = = = = = = = = = = = = = = = = = = = = =   POS start  = = = = = = = = = = = = = = = = = = = = = \n",
      "\n",
      ">>> Please input your sentence(if you want to quit, just type in quit):\n",
      "随着科技、经济、社会之间关系的日益密切，以及支持个人、组织、国家竞争的信息需求之日益迫切，信息分析突破了科技信息分析的范围，形成了众多分支领域，成为现代信息咨询业的重要组成部分。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1.WIN\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*  Sentence processing  -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      ">>> Processing input text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.886 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Segged text：   ['随着', '科技', '、', '经济', '、', '社会', '之间', '关系', '的', '日益', '密切', '，', '以及', '支持', '个人', '、', '组织', '、', '国家', '竞争', '的', '信息', '需求', '之', '日益', '迫切', '，', '信息', '分析', '突破', '了', '科技', '信息', '分析', '的', '范围', '，', '形成', '了', '众多', '分支', '领域', '，', '成为', '现代', '信息', '咨询业', '的', '重要', '组成部分', '。']\n",
      "\n",
      " -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*  POS output  -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      ">>> Using loaded model to predict...\n",
      "[{'随着': 'P'}, {'科技': 'NN'}, {'、': 'PU'}, {'经济': 'NN'}, {'、': 'PU'}, {'社会': 'NN'}, {'之间': 'LC'}, {'关系': 'NN'}, {'的': 'DEG'}, {'日益': 'NN'}, {'密切': 'VA'}, {'，': 'PU'}, {'以及': 'CC'}, {'支持': 'VV'}, {'个人': 'NN'}, {'、': 'PU'}, {'组织': 'NN'}, {'、': 'PU'}, {'国家': 'NN'}, {'竞争': 'NN'}, {'的': 'DEC'}, {'信息': 'NN'}, {'需求': 'NN'}, {'之': 'DEG'}, {'日益': 'NN'}, {'迫切': 'NN'}, {'，': 'PU'}, {'信息': 'NN'}, {'分析': 'NN'}, {'突破': 'VV'}, {'了': 'AS'}, {'科技': 'NN'}, {'信息': 'NN'}, {'分析': 'NN'}, {'的': 'DEC'}, {'范围': 'NN'}, {'，': 'PU'}, {'形成': 'VV'}, {'了': 'AS'}, {'众多': 'CD'}, {'分支': 'NN'}, {'领域': 'NN'}, {'，': 'PU'}, {'成为': 'VV'}, {'现代': 'JJ'}, {'信息': 'NN'}, {'咨询业': 'NN'}, {'的': 'DEG'}, {'重要': 'JJ'}, {'组成部分': 'NN'}, {'。': 'PU'}] \n",
      "\n",
      "\n",
      " = = = = = = = = = = = = = = = = = = = = =   POS start  = = = = = = = = = = = = = = = = = = = = = \n",
      "\n",
      ">>> Please input your sentence(if you want to quit, just type in quit):\n",
      "美国防政策委员会几位知名度较高的成员于当地时间26日被特朗普政府撤职。美媒称，这是特朗普政府对长期的外交政策专家和国家安全机构人士的又一次“清洗”。\n",
      "\n",
      " -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*  Sentence processing  -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      ">>> Processing input text...\n",
      ">>> Segged text：   ['美国防', '政策', '委员会', '几位', '知名度', '较', '高', '的', '成员', '于', '当地', '时间', '26', '日', '被', '特朗普', '政府', '撤职', '。', '美媒称', '，', '这是', '特朗普', '政府', '对', '长期', '的', '外交政策', '专家', '和', '国家', '安全', '机构', '人士', '的', '又', '一次', '“', '清洗', '”', '。']\n",
      "\n",
      " -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*  POS output  -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      ">>> Using loaded model to predict...\n",
      "[{'美国防': 'PU'}, {'政策': 'NN'}, {'委员会': 'NN'}, {'几位': 'NN'}, {'知名度': 'NN'}, {'较': 'AD'}, {'高': 'VA'}, {'的': 'DEC'}, {'成员': 'NN'}, {'于': 'P'}, {'当地': 'NN'}, {'时间': 'NN'}, {'26': 'CD'}, {'日': 'M'}, {'被': 'LB'}, {'特朗普': 'NR'}, {'政府': 'NN'}, {'撤职': 'VV'}, {'。': 'PU'}, {'美媒称': 'PU'}, {'，': 'PU'}, {'这是': 'PU'}, {'特朗普': 'NR'}, {'政府': 'NN'}, {'对': 'P'}, {'长期': 'NN'}, {'的': 'DEG'}, {'外交政策': 'NN'}, {'专家': 'NN'}, {'和': 'CC'}, {'国家': 'NN'}, {'安全': 'NN'}, {'机构': 'NN'}, {'人士': 'NN'}, {'的': 'DEG'}, {'又': 'AD'}, {'一次': 'NN'}, {'“': 'PU'}, {'清洗': 'VV'}, {'”': 'PU'}, {'。': 'PU'}] \n",
      "\n",
      "\n",
      " = = = = = = = = = = = = = = = = = = = = =   POS start  = = = = = = = = = = = = = = = = = = = = = \n",
      "\n",
      ">>> Please input your sentence(if you want to quit, just type in quit):\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 导入训练好的词性标注深度学习模型\n",
    "    print(\"= \"*20 + \" Loading model... \" + \"= \"*20)\n",
    "    pos_model = load_model(\"./cn_pos_tag_BiGRU.h5\")\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    \n",
    "    while True:\n",
    "        # 接收输入文本\n",
    "        print('\\n', '= '*21, ' POS start ', '= '*21)\n",
    "        text_to_be_tag = input(\"\\n>>> Please input your sentence(if you want to quit, just type in quit):\\n\")\n",
    "        if text_to_be_tag == 'quit':\n",
    "            break\n",
    "        print('\\n', '-*'*15, ' Sentence processing ', '-*'*15)\n",
    "        segged_text = pos_text_seg(text_to_be_tag)\n",
    "        \n",
    "        print('\\n', '-*'*17, ' POS output ', '-*'*17)\n",
    "        final_output = pos_text_predict(segged_text)\n",
    "        \n",
    "        print(final_output, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
