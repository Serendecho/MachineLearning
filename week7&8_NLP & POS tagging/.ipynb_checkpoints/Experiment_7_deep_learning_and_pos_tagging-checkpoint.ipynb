{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "train_data = './data/ctb5.1-pos/train.tsv'\n",
    "test_data = './data/ctb5.1-pos/test.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t', skip_blank_lines=False, header=None)\n",
    "    # 取出文本部分\n",
    "    content = data[0]\n",
    "    # 取出标签部分\n",
    "    label = data[1]\n",
    "    \n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "X_train, y_train = get_data(train_data)\n",
    "X_test, y_test = get_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理与格式转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520125 36 {nan: 18426, 'DER': 258, 'NP': 5, 'JJ': 13234, 'CS': 892, 'IJ': 12, 'FW': 33, 'X': 6, 'SP': 468, 'AD': 36430, 'VA': 7755, 'DEG': 12337, 'VV': 69858, 'VC': 5404, 'CC': 7355, 'PU': 76753, 'NN': 136643, 'BA': 755, 'VP': 1, 'OD': 1675, 'P': 17606, 'DEC': 12510, 'MSP': 1336, 'LC': 7782, 'DEV': 634, 'M': 13790, 'NT': 9659, 'CD': 16182, 'DT': 5986, 'VE': 3005, 'ETC': 1303, 'SB': 455, 'PN': 6644, 'LB': 245, 'AS': 4118, 'NR': 30570}\n"
     ]
    }
   ],
   "source": [
    "# 构建标签字典\n",
    "labels = y_train.tolist() + y_test.tolist()\n",
    "labels_types = list(set(labels))\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for label in labels_types:\n",
    "    labels_dict.update({label: labels.count(label)})\n",
    "print(len(labels), len(labels_dict), labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以句子进行拆分后的句子为：\n",
      " [array(['上海', '浦东', '开发', '与', '法制', '建设', '同步'], dtype=object), array(['新华社', '上海', '二月', '十日', '电', '（', '记者', '谢金虎', '、', '张持坚', '）'],\n",
      "      dtype=object), array(['上海', '浦东', '近年', '来', '颁布', '实行', '了', '涉及', '经济', '、', '贸易', '、',\n",
      "       '建设', '、', '规划', '、', '科技', '、', '文教', '等', '领域', '的', '七十一', '件',\n",
      "       '法规性', '文件', '，', '确保', '了', '浦东', '开发', '的', '有序', '进行', '。'],\n",
      "      dtype=object), array(['浦东', '开发', '开放', '是', '一', '项', '振兴', '上海', '，', '建设', '现代化',\n",
      "       '经济', '、', '贸易', '、', '金融', '中心', '的', '跨世纪', '工程', '，', '因此',\n",
      "       '大量', '出现', '的', '是', '以前', '不', '曾', '遇到', '过', '的', '新', '情况',\n",
      "       '、', '新', '问题', '。'], dtype=object), array(['对', '此', '，', '浦东', '不', '是', '简单', '的', '采取', '“', '干', '一', '段',\n",
      "       '时间', '，', '等', '积累', '了', '经验', '以后', '再', '制定', '法规', '条例', '”',\n",
      "       '的', '做法', '，', '而', '是', '借鉴', '发达', '国家', '和', '深圳', '等', '特区',\n",
      "       '的', '经验', '教训', '，', '聘请', '国内外', '有关', '专家', '学者', '，', '积极',\n",
      "       '、', '及时', '地', '制定', '和', '推出', '法规性', '文件', '，', '使', '这些', '经济',\n",
      "       '活动', '一', '出现', '就', '被', '纳入', '法制', '轨道', '。'], dtype=object)]\n",
      "以句子进行拆分后的句子所对应的词性为：\n",
      " [array(['NR', 'NR', 'NN', 'CC', 'NN', 'NN', 'VV'], dtype=object), array(['NN', 'NR', 'NT', 'NT', 'NN', 'PU', 'NN', 'NR', 'PU', 'NR', 'PU'],\n",
      "      dtype=object), array(['NR', 'NR', 'NT', 'LC', 'VV', 'VV', 'AS', 'VV', 'NN', 'PU', 'NN',\n",
      "       'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'ETC', 'NN', 'DEC',\n",
      "       'CD', 'M', 'NN', 'NN', 'PU', 'VV', 'AS', 'NR', 'NN', 'DEG', 'JJ',\n",
      "       'NN', 'PU'], dtype=object), array(['NR', 'NN', 'NN', 'VC', 'CD', 'M', 'VV', 'NR', 'PU', 'VV', 'NN',\n",
      "       'NN', 'PU', 'NN', 'PU', 'NN', 'NN', 'DEC', 'JJ', 'NN', 'PU', 'AD',\n",
      "       'AD', 'VV', 'DEC', 'VC', 'NT', 'AD', 'AD', 'VV', 'AS', 'DEC', 'JJ',\n",
      "       'NN', 'PU', 'JJ', 'NN', 'PU'], dtype=object), array(['P', 'PN', 'PU', 'NR', 'AD', 'VC', 'VA', 'DEV', 'VV', 'PU', 'VV',\n",
      "       'CD', 'M', 'NN', 'PU', 'P', 'VV', 'AS', 'NN', 'LC', 'AD', 'VV',\n",
      "       'NN', 'NN', 'PU', 'DEC', 'NN', 'PU', 'CC', 'VC', 'VV', 'JJ', 'NN',\n",
      "       'CC', 'NR', 'ETC', 'NN', 'DEG', 'NN', 'NN', 'PU', 'VV', 'NN', 'JJ',\n",
      "       'NN', 'NN', 'PU', 'AD', 'PU', 'AD', 'DEV', 'VV', 'CC', 'VV', 'NN',\n",
      "       'NN', 'PU', 'VV', 'DT', 'NN', 'NN', 'AD', 'VV', 'AD', 'SB', 'VV',\n",
      "       'NN', 'NN', 'PU'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# 按句对X、y进行拆分\n",
    "def split_corpus_by_sentence(content):\n",
    "    cleaned_sentence = []\n",
    "    split_label = content.isnull()\n",
    "    last_split_index = 0\n",
    "    index = 0\n",
    "    while index < len(content):\n",
    "        current_word = content[index]\n",
    "        if split_label[index] == True and len(cleaned_sentence) == 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index:index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "        elif split_label[index] == True  and len(cleaned_sentence) > 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index:index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "        else:\n",
    "            index += 1\n",
    "    return cleaned_sentence\n",
    "\n",
    "X_train_sent_split = split_corpus_by_sentence(X_train)\n",
    "y_train_sent_split = split_corpus_by_sentence(y_train)\n",
    "X_test_sent_split = split_corpus_by_sentence(X_test)\n",
    "y_test_sent_split = split_corpus_by_sentence(y_test)\n",
    "\n",
    "print('以句子进行拆分后的句子为：\\n', X_train_sent_split[:5])\n",
    "print('以句子进行拆分后的句子所对应的词性为：\\n', y_train_sent_split[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35, 35, 16, 14, 16, 16, 12], [16, 35, 26, 26, 16, 15, 16, 35, 15, 35, 15], [35, 35, 26, 23, 12, 12, 34, 12, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 30, 16, 21, 27, 25, 16, 16, 15, 12, 34, 35, 16, 11, 3, 16, 15], [35, 16, 16, 13, 27, 25, 12, 35, 15, 12, 16, 16, 15, 16, 15, 16, 16, 21, 3, 16, 15, 9, 9, 12, 21, 13, 26, 9, 9, 12, 34, 21, 3, 16, 15, 3, 16, 15], [20, 32, 15, 35, 9, 13, 10, 24, 12, 15, 12, 27, 25, 16, 15, 20, 12, 34, 16, 23, 9, 12, 16, 16, 15, 21, 16, 15, 14, 13, 12, 3, 16, 14, 35, 30, 16, 11, 16, 16, 15, 12, 16, 3, 16, 16, 15, 9, 15, 9, 24, 12, 14, 12, 16, 16, 15, 12, 28, 16, 16, 9, 12, 9, 31, 12, 16, 16, 15]]\n"
     ]
    }
   ],
   "source": [
    "# 将标签文本转换为索引（根据之前构建的标签字典），再转换为one-hot向量\n",
    "def transfer_label_category_index(origin_labels, labels_types):\n",
    "    transfered_label = []\n",
    "    for sentence_labels in origin_labels:\n",
    "        labels_format_index = [labels_types.index(label) for label in sentence_labels]  # 将标签依据字典转化为序号\n",
    "        transfered_label.append(labels_format_index)\n",
    "    return transfered_label\n",
    "\n",
    "y_train_index = transfer_label_category_index(y_train_sent_split, labels_types)\n",
    "y_test_index = transfer_label_category_index(y_test_sent_split, labels_types)\n",
    "\n",
    "print(y_train_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 设置句子最大长度\n",
    "MAX_SEQUENCE_LENGTH = 100 \n",
    "\n",
    "# 标签格式转化\n",
    "# 构建对应（标签样本数，句子长度，标签类别数）形状的张量，值全为0\n",
    "y_train_index_padded = np.zeros((len(y_train_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), dtype='float', order='C')\n",
    "y_test_index_padded = np.zeros((len(y_test_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), dtype='float', order='C')\n",
    "\n",
    "# 填充张量\n",
    "for sentence_labels_index in range(len(y_train_index)):\n",
    "    for label_index in range(len(y_train_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_train_index_padded[sentence_labels_index, label_index, y_train_index[sentence_labels_index][label_index]+1] = 1\n",
    "\n",
    "for sentence_labels_index in range(len(y_test_index)):\n",
    "    for label_index in range(len(y_test_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_test_index_padded[sentence_labels_index, label_index, y_test_index[sentence_labels_index][label_index]+1] = 1\n",
    "\n",
    "print(y_train_index_padded[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec模型导入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练的word2vec模型采用前人用中文维基百科训练好的模型，请各位同学进入链接下载，并放到相对本脚本 同一级文件夹data 的目录下 并解压。\n",
    "\n",
    "[word2vec模型链接](https://github.com/Embedding/Chinese-Word-Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "## 1 导入 预训练的词向量\n",
    "myPath = './data/sgns.wiki.word' # 本地词向量的地址\n",
    "Word2VecModel = gensim.models.KeyedVectors.load_word2vec_format(myPath).wv # 读取词向量，以二进制读取\n",
    "\n",
    "vector = Word2VecModel.wv['空间']  # 词语的向量，是numpy格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "，\n",
      "Vocab(count:352217, index:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(type(Word2VecModel.wv)) # 结果为：Word2VecKeyedVectors\n",
    "\n",
    "for i,j in Word2VecModel.wv.vocab.items():\n",
    "    print(i) # 此时 i 代表每个单词\n",
    "    print(j) # j 代表封装了 词频 等信息的 gensim“Vocab”对象，例子：Vocab(count:1481, index:38, sample_int:3701260191)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## 2 构造包含所有词语的 list，以及初始化 “词语-序号”字典 和 “词向量”矩阵\n",
    "vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]# 存储 所有的 词语\n",
    "\n",
    "word_index = {\" \": 0}# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "word_vector = {} # 初始化`[word : vector]`字典\n",
    "\n",
    "# 初始化存储所有向量的大矩阵，留意其中多一位（首行），词向量全为 0，用于 padding补零。\n",
    "# 行数 为 所有单词数+1 比如 10000+1 ； 列数为 词向量“维度”比如100。\n",
    "embeddings_matrix = np.zeros((len(vocab_list) + 1, Word2VecModel.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "## 3 填充 上述 的字典word_index 和 大矩阵\n",
    "for i in range(len(vocab_list)):\n",
    "    # print(i)\n",
    "    word = vocab_list[i]  # 每个词语\n",
    "    word_index[word] = i + 1 # 词语：序号\n",
    "    word_vector[word] = Word2VecModel.wv[word] # 词语：词向量\n",
    "    embeddings_matrix[i + 1] = Word2VecModel.wv[word]  # 词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  347 16980   507    10 15537   603  4380     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "# 序号化 文本，tokenizer句子，并返回每个句子所对应的词语索引\n",
    "\n",
    "# 由于将词语转化为索引的word_index需要与词向量模型对齐，故在导入词向量模型后再将X进行处理\n",
    "def tokenizer(texts, word_index):\n",
    "    data = []\n",
    "    for sentence in texts:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                new_sentence.append(word_index[word])  # 把文本中的 词语转化为index\n",
    "            except:\n",
    "                new_sentence.append(0)\n",
    "            \n",
    "        data.append(new_sentence)\n",
    "    # 使用kears的内置函数padding对齐句子,好处是输出numpy数组，不用自己转化了\n",
    "    data = sequence.pad_sequences(data, maxlen = MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    return data\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train_sent_split, word_index)\n",
    "X_test_tokenized = tokenizer(X_test_sent_split, word_index)\n",
    "\n",
    "print(X_train_tokenized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标引网络构建及训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import keras\n",
    "\n",
    "EMBEDDING_DIM = 300 # 词向量维度\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） \n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                   ))\n",
    "# input shape (Batch_size, Time_step, Input_Sizes)\n",
    "model.add(LSTM(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), activation='tanh', return_sequences=True)) # 增加bidirectional\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, input_shape=(MAX_SEQUENCE_LENGTH, 128), activation='relu')) # 128维\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels_types)+1, input_shape=(MAX_SEQUENCE_LENGTH, 64), activation='softmax')) # 64维\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',  # 损失函数\n",
    "              optimizer='adam',  # 优化器——更改参数\n",
    "              metrics=['accuracy']  # 评价指标\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          105665400 \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 64)           8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100, 37)           2405      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 37)           0         \n",
      "=================================================================\n",
      "Total params: 105,895,709\n",
      "Trainable params: 230,309\n",
      "Non-trainable params: 105,665,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18078, 100) (18078, 100, 37)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tokenized.shape, y_train_index_padded.shape) # 18078个样本,每个样本100个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/slyfox/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "18078/18078 [==============================] - 54s 3ms/step - loss: 2.3866 - accuracy: 0.0789\n",
      "Epoch 2/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2724 - accuracy: 0.1115\n",
      "Epoch 3/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2541 - accuracy: 0.1170\n",
      "Epoch 4/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2439 - accuracy: 0.1191\n",
      "Epoch 5/20\n",
      "18078/18078 [==============================] - 52s 3ms/step - loss: 2.2488 - accuracy: 0.1198\n",
      "Epoch 6/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2436 - accuracy: 0.1208\n",
      "Epoch 7/20\n",
      "18078/18078 [==============================] - 54s 3ms/step - loss: 2.2364 - accuracy: 0.1218\n",
      "Epoch 8/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2378 - accuracy: 0.1223\n",
      "Epoch 9/20\n",
      "18078/18078 [==============================] - 55s 3ms/step - loss: 2.2352 - accuracy: 0.1226\n",
      "Epoch 10/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2288 - accuracy: 0.1233\n",
      "Epoch 11/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2333 - accuracy: 0.1233\n",
      "Epoch 12/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2248 - accuracy: 0.1241\n",
      "Epoch 13/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2294 - accuracy: 0.1240\n",
      "Epoch 14/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2301 - accuracy: 0.1242\n",
      "Epoch 15/20\n",
      "18078/18078 [==============================] - 52s 3ms/step - loss: 2.2321 - accuracy: 0.1242\n",
      "Epoch 16/20\n",
      "18078/18078 [==============================] - 54s 3ms/step - loss: 2.2284 - accuracy: 0.1246\n",
      "Epoch 17/20\n",
      "18078/18078 [==============================] - 53s 3ms/step - loss: 2.2261 - accuracy: 0.1248\n",
      "Epoch 18/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2273 - accuracy: 0.1250\n",
      "Epoch 19/20\n",
      "18078/18078 [==============================] - 52s 3ms/step - loss: 2.2242 - accuracy: 0.1252\n",
      "Epoch 20/20\n",
      "18078/18078 [==============================] - 51s 3ms/step - loss: 2.2249 - accuracy: 0.1253\n",
      "evaluation!\n",
      "348/348 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_tokenized, y_train_index_padded,\n",
    "          epochs=20, # 20轮\n",
    "          batch_size=128,\n",
    "          verbose=1  # 展示训练过程\n",
    "         )\n",
    "print('evaluation!')\n",
    "score = model.evaluate(X_test_tokenized, y_test_index_padded, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练的模型经在测试集上验证获得的loss和accuracy为：\n",
      "[0.05799064429840822, 0.20675286650657654]\n"
     ]
    }
   ],
   "source": [
    "print('训练的模型经在测试集上验证获得的loss和accuracy为：')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5.2290096e-07 2.6394284e-07 1.4585145e-06 ... 5.7066409e-06\n",
      "   9.5998666e-06 6.9099808e-01]\n",
      "  [1.9605020e-10 2.8755645e-11 4.7406906e-10 ... 1.4757541e-11\n",
      "   3.4634517e-10 9.8503667e-01]\n",
      "  [5.1990853e-11 3.0280665e-12 1.4870039e-09 ... 8.3266883e-08\n",
      "   7.4978939e-09 5.9678172e-07]\n",
      "  ...\n",
      "  [5.2306046e-07 2.9986347e-07 3.8933899e-06 ... 5.3384105e-07\n",
      "   5.6671212e-07 1.9845031e-02]\n",
      "  [5.2306046e-07 2.9986347e-07 3.8933899e-06 ... 5.3384105e-07\n",
      "   5.6671212e-07 1.9845031e-02]\n",
      "  [5.2306046e-07 2.9986347e-07 3.8933940e-06 ... 5.3384105e-07\n",
      "   5.6671212e-07 1.9845037e-02]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test_tokenized[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3.45361896e-14 4.47106892e-15 4.74992875e-13 ... 1.62147195e-14\n",
      "   8.04468186e-14 1.36462643e-06]\n",
      "  [1.76132282e-08 1.17716965e-08 4.95060846e-08 ... 4.80711542e-08\n",
      "   1.13095993e-07 9.02875662e-01]\n",
      "  [2.06781068e-11 2.86890932e-12 5.92366711e-11 ... 1.52504159e-12\n",
      "   4.57937646e-11 9.90172505e-01]\n",
      "  ...\n",
      "  [5.23060464e-07 2.99863473e-07 3.89339402e-06 ... 5.33841501e-07\n",
      "   5.66712117e-07 1.98450424e-02]\n",
      "  [5.23060464e-07 2.99863473e-07 3.89338993e-06 ... 5.33841046e-07\n",
      "   5.66712117e-07 1.98450424e-02]\n",
      "  [5.23059953e-07 2.99862904e-07 3.89338993e-06 ... 5.33841046e-07\n",
      "   5.66712117e-07 1.98450424e-02]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test_tokenized[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_index_padded[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
