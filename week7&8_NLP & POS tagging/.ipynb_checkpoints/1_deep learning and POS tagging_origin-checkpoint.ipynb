{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习进行词性标注（原版）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# 训练集和测试集的地址\n",
    "train_data = './data/ctb5.1-pos/train.tsv'\n",
    "test_data = './data/ctb5.1-pos/test.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据读取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集，并提取出其中的文本部分和标签部分\n",
    "def get_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t', \n",
    "                       skip_blank_lines=False, \n",
    "                       header=None,\n",
    "                      )\n",
    "    \n",
    "    # 取出文本部分\n",
    "    content = data[0]\n",
    "    \n",
    "    # 取出标签部分\n",
    "    label = data[1]\n",
    "    \n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用get_data函数获取训练集和测试集的文本和标签\n",
    "X_train, y_train = get_data(train_data)\n",
    "X_test, y_test = get_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据预处理与格式转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集和测试集的总标签数len(labels)为：520125\n",
      "训练集和测试集的总标签类别数len(labels_types)为：36\n",
      "标签字典的对象数len(labels_dict)为：36\n",
      "标签字典内容labels_dict为：\n",
      "{nan: 18426, 'M': 13790, 'DT': 5986, 'PN': 6644, 'OD': 1675, 'VP': 1, 'JJ': 13234, 'VA': 7755, 'BA': 755, 'NT': 9659, 'VE': 3005, 'SP': 468, 'CS': 892, 'MSP': 1336, 'FW': 33, 'NP': 5, 'DEC': 12510, 'AD': 36430, 'AS': 4118, 'DEG': 12337, 'P': 17606, 'PU': 76753, 'CC': 7355, 'ETC': 1303, 'DER': 258, 'CD': 16182, 'LB': 245, 'IJ': 12, 'LC': 7782, 'VV': 69858, 'X': 6, 'NR': 30570, 'DEV': 634, 'NN': 136643, 'SB': 455, 'VC': 5404}\n"
     ]
    }
   ],
   "source": [
    "## 构建标签字典\n",
    "# 合并训练集和测试集的标签\n",
    "labels = y_train.tolist() + y_test.tolist()\n",
    "# 利用集合进行去重，再转化为列表格式，获得标签类型列表\n",
    "labels_types = list(set(labels))\n",
    "print(\"训练集和测试集的总标签类别数len(labels_types)为：\" + str(len(labels_types)))\n",
    "print(\"标签类型列表为：\\n\", labels_types)\n",
    "\n",
    "# 构建标签频次字典（标签：该标签出现次数）\n",
    "labels_dict = {}\n",
    "\n",
    "# 构建标签索引字典（标签： 该标签的索引）\n",
    "# 加入{\"padded_label\" : 0}降低损失率\n",
    "labels_index = {\"padded_label\" : 0}\n",
    "\n",
    "for index in range(len(labels_types)):\n",
    "    # 根据标签类别列表通过索引获取标签\n",
    "    label = labels_types[index]\n",
    "    # 更新标签频次字典\n",
    "    labels_dict.update({label: labels.count(label)}) \n",
    "    # 更新标签索引字典\n",
    "    labels_index.update({label: index+1}) \n",
    "    \n",
    "print(\"\\n训练集和测试集的总标签数len(labels)为：\" + str(len(labels)))\n",
    "\n",
    "# 输出构建好的标签频次字典\n",
    "print(\"\\n标签字典的对象数len(labels_dict)为：\" + str(len(labels_dict)))\n",
    "print(\"标签频次字典内容labels_dict为：\\n\", labels_dict)\n",
    "\n",
    "# 输出构建好的标签索引字典\n",
    "print(\"\\n标签索引字典内容labels_dict为：\\n\", labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中按句进行拆分后的句子为：\n",
      " [array(['上海', '浦东', '开发', '与', '法制', '建设', '同步'], dtype=object), array(['新华社', '上海', '二月', '十日', '电', '（', '记者', '谢金虎', '、', '张持坚', '）'],\n",
      "      dtype=object), array(['上海', '浦东', '近年', '来', '颁布', '实行', '了', '涉及', '经济', '、', '贸易', '、',\n",
      "       '建设', '、', '规划', '、', '科技', '、', '文教', '等', '领域', '的', '七十一', '件',\n",
      "       '法规性', '文件', '，', '确保', '了', '浦东', '开发', '的', '有序', '进行', '。'],\n",
      "      dtype=object), array(['浦东', '开发', '开放', '是', '一', '项', '振兴', '上海', '，', '建设', '现代化',\n",
      "       '经济', '、', '贸易', '、', '金融', '中心', '的', '跨世纪', '工程', '，', '因此',\n",
      "       '大量', '出现', '的', '是', '以前', '不', '曾', '遇到', '过', '的', '新', '情况',\n",
      "       '、', '新', '问题', '。'], dtype=object), array(['对', '此', '，', '浦东', '不', '是', '简单', '的', '采取', '“', '干', '一', '段',\n",
      "       '时间', '，', '等', '积累', '了', '经验', '以后', '再', '制定', '法规', '条例', '”',\n",
      "       '的', '做法', '，', '而', '是', '借鉴', '发达', '国家', '和', '深圳', '等', '特区',\n",
      "       '的', '经验', '教训', '，', '聘请', '国内外', '有关', '专家', '学者', '，', '积极',\n",
      "       '、', '及时', '地', '制定', '和', '推出', '法规性', '文件', '，', '使', '这些', '经济',\n",
      "       '活动', '一', '出现', '就', '被', '纳入', '法制', '轨道', '。'], dtype=object)]\n",
      "\n",
      "训练集中按句进行拆分后的句子所对应的词性为：\n",
      " [array(['NR', 'NR', 'NN', 'CC', 'NN', 'NN', 'VV'], dtype=object), array(['NN', 'NR', 'NT', 'NT', 'NN', 'PU', 'NN', 'NR', 'PU', 'NR', 'PU'],\n",
      "      dtype=object), array(['NR', 'NR', 'NT', 'LC', 'VV', 'VV', 'AS', 'VV', 'NN', 'PU', 'NN',\n",
      "       'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'ETC', 'NN', 'DEC',\n",
      "       'CD', 'M', 'NN', 'NN', 'PU', 'VV', 'AS', 'NR', 'NN', 'DEG', 'JJ',\n",
      "       'NN', 'PU'], dtype=object), array(['NR', 'NN', 'NN', 'VC', 'CD', 'M', 'VV', 'NR', 'PU', 'VV', 'NN',\n",
      "       'NN', 'PU', 'NN', 'PU', 'NN', 'NN', 'DEC', 'JJ', 'NN', 'PU', 'AD',\n",
      "       'AD', 'VV', 'DEC', 'VC', 'NT', 'AD', 'AD', 'VV', 'AS', 'DEC', 'JJ',\n",
      "       'NN', 'PU', 'JJ', 'NN', 'PU'], dtype=object), array(['P', 'PN', 'PU', 'NR', 'AD', 'VC', 'VA', 'DEV', 'VV', 'PU', 'VV',\n",
      "       'CD', 'M', 'NN', 'PU', 'P', 'VV', 'AS', 'NN', 'LC', 'AD', 'VV',\n",
      "       'NN', 'NN', 'PU', 'DEC', 'NN', 'PU', 'CC', 'VC', 'VV', 'JJ', 'NN',\n",
      "       'CC', 'NR', 'ETC', 'NN', 'DEG', 'NN', 'NN', 'PU', 'VV', 'NN', 'JJ',\n",
      "       'NN', 'NN', 'PU', 'AD', 'PU', 'AD', 'DEV', 'VV', 'CC', 'VV', 'NN',\n",
      "       'NN', 'PU', 'VV', 'DT', 'NN', 'NN', 'AD', 'VV', 'AD', 'SB', 'VV',\n",
      "       'NN', 'NN', 'PU'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# 按句子对X、y进行拆分\n",
    "def split_corpus_by_sentence(content):\n",
    "    cleaned_sentence = []\n",
    "    split_label = content.isnull() # 用来判断是否有缺失值，返回布尔值\n",
    "    last_split_index = 0\n",
    "    index = 0\n",
    "    \n",
    "    while index < len(content):\n",
    "        current_word = content[index]\n",
    "        # 如果有缺失值且cleaned_sentence列表中无内容\n",
    "        if split_label[index] == True and len(cleaned_sentence) == 0:\n",
    "            # 添加content列表中从索引last_split_index到index的内容到cleaned_sentence\n",
    "            cleaned_sentence.append(np.array(content[last_split_index: index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "            \n",
    "        # 如果有缺失值且cleaned_sentence列表中有内容\n",
    "        elif split_label[index] == True and len(cleaned_sentence) > 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index: index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "            \n",
    "        else:\n",
    "            index += 1\n",
    "    return cleaned_sentence\n",
    "\n",
    "# 利用上述函数按句子拆分训练集和测试集的文本和标签\n",
    "X_train_sent_split = split_corpus_by_sentence(X_train)\n",
    "y_train_sent_split = split_corpus_by_sentence(y_train)\n",
    "X_test_sent_split = split_corpus_by_sentence(X_test)\n",
    "y_test_sent_split = split_corpus_by_sentence(y_test)\n",
    "\n",
    "print('训练集中按句进行拆分后的句子为：\\n', X_train_sent_split[:5])\n",
    "print('\\n训练集中按句进行拆分后的句子所对应的词性为：\\n', y_train_sent_split[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中的标签转换为索引对应为：\n",
      " [[31, 31, 33, 22, 33, 33, 29], [33, 31, 9, 9, 33, 21, 33, 31, 21, 31, 21], [31, 31, 9, 28, 29, 29, 18, 29, 33, 21, 33, 21, 33, 21, 33, 21, 33, 21, 33, 23, 33, 16, 25, 1, 33, 33, 21, 29, 18, 31, 33, 19, 6, 33, 21], [31, 33, 33, 35, 25, 1, 29, 31, 21, 29, 33, 33, 21, 33, 21, 33, 33, 16, 6, 33, 21, 17, 17, 29, 16, 35, 9, 17, 17, 29, 18, 16, 6, 33, 21, 6, 33, 21], [20, 3, 21, 31, 17, 35, 7, 32, 29, 21, 29, 25, 1, 33, 21, 20, 29, 18, 33, 28, 17, 29, 33, 33, 21, 16, 33, 21, 22, 35, 29, 6, 33, 22, 31, 23, 33, 19, 33, 33, 21, 29, 33, 6, 33, 33, 21, 17, 21, 17, 32, 29, 22, 29, 33, 33, 21, 29, 2, 33, 33, 17, 29, 17, 34, 29, 33, 33, 21]]\n",
      "\n",
      "测试集中的标签转换为索引对应为：\n",
      " [[31, 31, 20, 31, 29, 6, 33, 33, 33], [31, 31, 9, 9, 33, 21, 33, 31, 21, 31, 21], [21, 31, 31, 33, 6, 33, 33, 33, 33, 21, 9, 20, 31, 29, 21], [9, 20, 3, 29, 16, 35, 33, 33, 33, 33, 33, 22, 33, 33, 33, 25, 1, 33, 21, 17, 17, 29, 18, 33, 33, 33, 33, 33, 21], [2, 25, 1, 33, 35, 17, 20, 33, 33, 33, 33, 33, 33, 33, 33, 31, 33, 22, 31, 31, 6, 33, 33, 21, 20, 31, 33, 33, 33, 33, 21, 33, 33, 33, 21, 31, 33, 29, 16, 21]]\n"
     ]
    }
   ],
   "source": [
    "# 根据之前构建的标签字典将标签文本转换为索引\n",
    "def transfer_label_category_index(origin_labels, labels_types):\n",
    "    transfered_label = []\n",
    "    for sentence_labels in origin_labels:\n",
    "        # 将标签依据字典转换为序号\n",
    "        labels_format_index = [labels_types.index(label) for label in sentence_labels]\n",
    "        transfered_label.append(labels_format_index)\n",
    "    return transfered_label\n",
    "\n",
    "# 利用上述函数将训练集和测试集的标签转换为索引\n",
    "y_train_index = transfer_label_category_index(y_train_sent_split, labels_types)\n",
    "y_test_index = transfer_label_category_index(y_test_sent_split, labels_types)\n",
    "\n",
    "print('训练集中的标签转换为索引对应为：\\n', y_train_index[:5])\n",
    "print('\\n测试集中的标签转换为索引对应为：\\n', y_test_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中填充的张量为：\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "测试集中填充的张量为：\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 设置句子的最大长度为100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "## 进行标签格式转化\n",
    "# 利用zeros(shape, dtype=float, order='C')构建张量，值全为0\n",
    "# 形状shape对应（标签样本数，句子长度，标签类别数）\n",
    "y_train_index_padded = np.zeros((len(y_train_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), # 形状 \n",
    "                                dtype='float', # 数据类型 \n",
    "                                order='C',     # c代表与c语言类似，行优先\n",
    "                               )\n",
    "y_test_index_padded = np.zeros((len(y_test_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), # 形状 \n",
    "                              dtype='float', # 数据类型 \n",
    "                              order='C',     # c代表与c语言类似，行优先\n",
    "                              )\n",
    "\n",
    "''' \n",
    "填充张量\n",
    "嵌套循环遍历：先句子后词\n",
    "'''\n",
    "# 训练集\n",
    "for sentence_labels_index in range(len(y_train_index)):\n",
    "    for label_index in range(len(y_train_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_train_index_padded[sentence_labels_index, label_index, y_train_index[sentence_labels_index][label_index]+1] = 1\n",
    "    # 优化：若为填充的标签，则将其预测为第一位为1        \n",
    "    if len(y_train_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_train_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_train_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "\n",
    "# 测试集      \n",
    "for sentence_labels_index in range(len(y_test_index)):\n",
    "    for label_index in range(len(y_test_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_test_index_padded[sentence_labels_index, label_index, y_test_index[sentence_labels_index][label_index]+1] = 1        \n",
    "    # 优化：若为填充的标签，则将其预测为第一位为1 \n",
    "    if len(y_test_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_test_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_test_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "    \n",
    "print('训练集中填充的张量为：\\n', y_train_index_padded[:1])\n",
    "print('\\n测试集中填充的张量为：\\n', y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. word2vec模型导入\n",
    "预训练的word2vec模型采用前人用中文维基百科训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "'''（1）导入 预训练的词向量'''\n",
    "\n",
    "# 本地词向量的地址\n",
    "myPath = './data/sgns.wiki.word'   \n",
    "\n",
    "# 以二进制读取词向量\n",
    "Word2VecModel = gensim.models.KeyedVectors.load_word2vec_format(myPath).wv\n",
    "\n",
    "# 词语的向量，是numpy格式\n",
    "vector = Word2VecModel.wv['空间']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的类型为： <class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "，\n",
      "Vocab(count:352217, index:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 输出词向量的类型\n",
    "print('词向量的类型为：', type(Word2VecModel.wv))  \n",
    "# 结果为：Word2VecKeyedVectors\n",
    "\n",
    "# 获取word2vec训练后model中的所有的词\n",
    "for i, j in Word2VecModel.wv.vocab.items():\n",
    "    # 此时 i 代表每个单词\n",
    "    print(i) \n",
    "    \n",
    "    # j 代表封装了 词频 等信息的 gensim“Vocab”对象\n",
    "    # 例子：Vocab(count:1481, index:38, sample_int:3701260191)\n",
    "    print(j)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "'''（2）构造包含所有词语的list；\n",
    "        初始化“词语-序号”字典和“词向量”矩阵\n",
    "'''\n",
    "# 存储所有的单词和gensim“Vocab”对象\n",
    "vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]\n",
    "\n",
    "# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "word_index = {\" \": 0}\n",
    "\n",
    "# 初始化`[word : vector]`字典\n",
    "word_vector = {}\n",
    "\n",
    "# 初始化 存储所有向量的大矩阵【其中多一位（首行）：词向量全为 0，用于 padding补零。\n",
    "# 行数为：所有单词数+1。比如 10000+1 ； \n",
    "# 列数为：词向量“维度”。比如100。\n",
    "embeddings_matrix = np.zeros((len(vocab_list)+1, Word2VecModel.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "'''（3）填充上述的字典word_index和大矩阵embeddings_matrix'''\n",
    "\n",
    "# 循环遍历 存储所有的单词和gensim“Vocab”对象的 vocab_list列表\n",
    "for i in range(len(vocab_list)):\n",
    "    #print(i)    \n",
    "    # 将每个词语存储到vocab_list列表中\n",
    "    word = vocab_list[i]    \n",
    "    # 将每个词语及对应的序号存储到word_index字典中\n",
    "    word_index[word] = i + 1   \n",
    "    # 将每个{词语：词向量}存储到word_vector字典中\n",
    "    word_vector[word] = Word2VecModel.wv[word]   \n",
    "    # 将每个{词向量：序号+1}存储到词向量矩阵embeddings_matrix中\n",
    "    embeddings_matrix[i + 1] = Word2VecModel.wv[word]\n",
    "    \n",
    "np.save('x_word_index.npy', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中的词语转换为索引为：\n",
      " [[  347 16980   507    10 15537   603  4380     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "测试集中的词语转换为索引为：\n",
      " [[  28  523    6 5705 2208  287 1216  587 1523    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "'''（4）序号化 文本；\n",
    "        tokenizer句子；\n",
    "        并返回每个句子所对应的词语索引\n",
    "'''\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# 由于将词语转化为索引的word_index需要与词向量模型对齐，故在导入词向量模型后再将X进行处理\n",
    "def tokenizer(texts, word_index):\n",
    "    data = []\n",
    "    \n",
    "    for sentence in texts:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                # 根据word_index字典把文本中的词语转化为index\n",
    "                new_sentence.append(word_index[word])\n",
    "            except:\n",
    "                new_sentence.append(0)\n",
    "        data.append(new_sentence)\n",
    "        \n",
    "    # 使用kears的内置函数padding对齐句子（好处是输出numpy数组，不用自己转化了）\n",
    "    data = sequence.pad_sequences(data,\n",
    "                                 maxlen = MAX_SEQUENCE_LENGTH, # 序列的最大长度（大于此长度的序列将被截短，小于此长度的序列将在后部填0）\n",
    "                                 padding = 'post', # 当需要补0时，在序列的结尾补\n",
    "                                 truncating = 'post', # 当需要截断序列时，从结尾截断\n",
    "                                 )\n",
    "    return data\n",
    "\n",
    "# 将训练集中的词语转换为索引\n",
    "X_train_tokenized = tokenizer(X_train_sent_split, word_index)\n",
    "\n",
    "# 将测试集中的词语转换为索引\n",
    "X_test_tokenized = tokenizer(X_test_sent_split, word_index)\n",
    "\n",
    "print('训练集中的词语转换为索引为：\\n', X_train_tokenized[:1])\n",
    "print('\\n测试集中的词语转换为索引为：\\n', X_test_tokenized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 标引网络构建及训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import keras\n",
    "\n",
    "# 词向量维度为300\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量长度（300）\n",
    "                    weights = [embeddings_matrix], # 预训练的词向量系数\n",
    "                    input_length = MAX_SEQUENCE_LENGTH, # 每句话的最大长度（必须padding） \n",
    "                    trainable = False, # 是否在训练的过程中更新词向量\n",
    "                   ),\n",
    "         )\n",
    "\n",
    "## input_shape=(Batch_size, Time_step, Input_Sizes)\n",
    "# 输入层\n",
    "# 可以增加bidirectional改变效果\n",
    "model.add(LSTM(128, \n",
    "               input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "               activation='tanh', # 激活函数\n",
    "               return_sequences=True, # 返回全部time step的hidden state值\n",
    "              )\n",
    "         )\n",
    "model.add(Dropout(0.5)) # 减少过拟合\n",
    "\n",
    "# 隐藏层\n",
    "model.add(Dense(64, \n",
    "                input_shape=(MAX_SEQUENCE_LENGTH, 128), # 128维\n",
    "                activation='relu',\n",
    "               )\n",
    "         )\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 输出层\n",
    "model.add(Dense(len(labels_types)+1,\n",
    "                input_shape=(MAX_SEQUENCE_LENGTH, 64),  # 64维\n",
    "                activation='softmax',\n",
    "               )\n",
    "         )\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # 损失函数\n",
    "              optimizer='adam', # 优化器（可以更改优化器参数进行优化）\n",
    "              metrics=['accuracy'], # 评价指标是准确率\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          105665400 \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100, 64)           8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 37)           2405      \n",
      "=================================================================\n",
      "Total params: 105,895,709\n",
      "Trainable params: 230,309\n",
      "Non-trainable params: 105,665,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 输出模型各层的参数状况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中将词语转换为索引后: (18078, 100)\n",
      "\n",
      "训练集中将标签转换为索引并填充张量后 (18078, 100, 37)\n"
     ]
    }
   ],
   "source": [
    "# 输出训练集中将词语转换为索引后的shape\n",
    "print('训练集中将词语转换为索引后:', X_train_tokenized.shape)\n",
    "# 18078个样本,每个样本100个词\n",
    "\n",
    "# 输出训练集中将标签转换为索引并填充张量后的shape\n",
    "print('\\n训练集中将标签转换为索引并填充张量后', y_train_index_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 43s 302ms/step - loss: 0.4880 - accuracy: 0.1398\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 42s 296ms/step - loss: 0.2078 - accuracy: 0.2110\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 42s 293ms/step - loss: 0.1674 - accuracy: 0.2216\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 42s 295ms/step - loss: 0.1509 - accuracy: 0.2262\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 42s 294ms/step - loss: 0.1406 - accuracy: 0.2288\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 44s 313ms/step - loss: 0.1331 - accuracy: 0.2309\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 41s 288ms/step - loss: 0.1273 - accuracy: 0.2325\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 43s 305ms/step - loss: 0.1221 - accuracy: 0.2340\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 42s 294ms/step - loss: 0.1180 - accuracy: 0.2351\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 42s 293ms/step - loss: 0.1148 - accuracy: 0.2362\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 42s 295ms/step - loss: 0.1117 - accuracy: 0.2369\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 42s 297ms/step - loss: 0.1091 - accuracy: 0.2377\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 43s 304ms/step - loss: 0.1064 - accuracy: 0.2385\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 44s 308ms/step - loss: 0.1043 - accuracy: 0.2391\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 53s 373ms/step - loss: 0.1023 - accuracy: 0.2396\n",
      "Epoch 16/20\n",
      "142/142 [==============================] - 55s 388ms/step - loss: 0.1009 - accuracy: 0.2402\n",
      "Epoch 17/20\n",
      "142/142 [==============================] - 51s 359ms/step - loss: 0.0992 - accuracy: 0.2406\n",
      "Epoch 18/20\n",
      "142/142 [==============================] - 48s 339ms/step - loss: 0.0975 - accuracy: 0.2411\n",
      "Epoch 19/20\n",
      "142/142 [==============================] - 52s 363ms/step - loss: 0.0960 - accuracy: 0.2415\n",
      "Epoch 20/20\n",
      "142/142 [==============================] - 53s 370ms/step - loss: 0.0947 - accuracy: 0.2421\n",
      "-------------------Evaluation------------------------\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.0533 - accuracy: 0.2089\n"
     ]
    }
   ],
   "source": [
    "# 使用训练集对模型进行训练\n",
    "model.fit(X_train_tokenized,\n",
    "          y_train_index_padded,\n",
    "          epochs = 20,  # 训练20轮\n",
    "          batch_size = 128, # 指定进行梯度下降时每个batch包含的样本数为128\n",
    "          verbose = 1, # 展示训练过程\n",
    "         )\n",
    "\n",
    "# 评估训练完成的模型.返回损失值&模型的度量值.\n",
    "print('-------------------Evaluation------------------------')\n",
    "score = model.evaluate(X_test_tokenized, \n",
    "                       y_test_index_padded,\n",
    "                       batch_size=128,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存adam_origin模型\n",
    "model.save('cn_pos_tag_adam_origin.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练的模型经在测试集上验证获得的loss和accuracy为：\n",
      "[0.05334566906094551, 0.20887930691242218]\n"
     ]
    }
   ],
   "source": [
    "print('训练的模型经在测试集上验证获得的loss和accuracy为：')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------对测试集进行预测------------------------\n",
      "（1）对测试集 X_test_tokenized[:1] 进行预测：\n",
      "[[[1.74216484e-06 9.23137009e-07 2.28906865e-05 ... 3.00220922e-02\n",
      "   2.02601914e-05 6.78766301e-05]\n",
      "  [4.41569420e-10 2.86696389e-10 1.26233255e-08 ... 6.34323526e-03\n",
      "   1.29417943e-08 3.56986298e-08]\n",
      "  [1.42245308e-12 3.32192563e-12 1.92941507e-09 ... 5.04353557e-06\n",
      "   1.21673852e-10 5.99882640e-07]\n",
      "  ...\n",
      "  [7.38082377e-08 2.37971030e-08 1.03065831e-05 ... 1.14819752e-02\n",
      "   5.56981874e-08 2.26892254e-07]\n",
      "  [7.38082377e-08 2.37971030e-08 1.03065831e-05 ... 1.14819752e-02\n",
      "   5.56981874e-08 2.26892254e-07]\n",
      "  [7.38080956e-08 2.37971030e-08 1.03065740e-05 ... 1.14819752e-02\n",
      "   5.56981874e-08 2.26892254e-07]]]\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行预测\n",
    "print('-----------------------对测试集进行预测------------------------')\n",
    "print('（1）对测试集 X_test_tokenized[:1] 进行预测：')\n",
    "print(model.predict(X_test_tokenized[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中标签填充的张量 y_test_index_padded[:1] 为：\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 输出测试集中标签填充的张量\n",
    "print('测试集中标签填充的张量 y_test_index_padded[:1] 为：')\n",
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（2）对测试集 X_test_tokenized[2: 3] 进行预测：\n",
      "[[[5.6089702e-14 6.1985274e-15 2.9637137e-10 ... 4.3899840e-06\n",
      "   9.0499592e-15 1.3664829e-11]\n",
      "  [2.0207594e-07 7.8966558e-08 2.1363035e-06 ... 2.8190421e-02\n",
      "   1.9605411e-06 6.7759497e-06]\n",
      "  [6.3099456e-11 3.3175313e-11 2.5033946e-09 ... 4.2244354e-03\n",
      "   1.9777437e-09 4.6387907e-09]\n",
      "  ...\n",
      "  [7.3808941e-08 2.3797375e-08 1.0306663e-05 ... 1.1482009e-02\n",
      "   5.5698823e-08 2.2689507e-07]\n",
      "  [7.3808799e-08 2.3797329e-08 1.0306643e-05 ... 1.1481998e-02\n",
      "   5.5698610e-08 2.2689441e-07]\n",
      "  [7.3808664e-08 2.3797284e-08 1.0306632e-05 ... 1.1482009e-02\n",
      "   5.5698610e-08 2.2689399e-07]]]\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行预测\n",
    "print('（2）对测试集 X_test_tokenized[2: 3] 进行预测：')\n",
    "print(model.predict(X_test_tokenized[2: 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中标签填充的张量 y_test_index_padded[:1] 为：\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 输出测试集中标签填充的张量\n",
    "print('测试集中标签填充的张量 y_test_index_padded[:1] 为：')\n",
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
