{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习进行词性标注（优化器改为SGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# 训练集和测试集的地址\n",
    "train_data = './data/ctb5.1-pos/train.tsv'\n",
    "test_data = './data/ctb5.1-pos/test.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据读取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集，并提取出其中的文本部分和标签部分\n",
    "def get_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t', \n",
    "                       skip_blank_lines=False, \n",
    "                       header=None,\n",
    "                      )\n",
    "    \n",
    "    # 取出文本部分\n",
    "    content = data[0]\n",
    "    \n",
    "    # 取出标签部分\n",
    "    label = data[1]\n",
    "    \n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用get_data函数获取训练集和测试集的文本和标签\n",
    "X_train, y_train = get_data(train_data)\n",
    "X_test, y_test = get_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据预处理与格式转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集和测试集的总标签类别数len(labels_types)为：36\n",
      "标签类型列表为：\n",
      " [nan, 'VV', 'DEG', 'IJ', 'NR', 'CS', 'SP', 'VA', 'FW', 'PN', 'NT', 'BA', 'P', 'AD', 'ETC', 'X', 'VC', 'VE', 'NP', 'DT', 'CC', 'AS', 'SB', 'DER', 'M', 'OD', 'CD', 'LB', 'PU', 'VP', 'LC', 'MSP', 'JJ', 'DEV', 'NN', 'DEC']\n",
      "\n",
      "训练集和测试集的总标签数len(labels)为：520125\n",
      "\n",
      "标签字典的对象数len(labels_dict)为：36\n",
      "标签频次字典内容labels_dict为：\n",
      " {nan: 18426, 'VV': 69858, 'DEG': 12337, 'IJ': 12, 'NR': 30570, 'CS': 892, 'SP': 468, 'VA': 7755, 'FW': 33, 'PN': 6644, 'NT': 9659, 'BA': 755, 'P': 17606, 'AD': 36430, 'ETC': 1303, 'X': 6, 'VC': 5404, 'VE': 3005, 'NP': 5, 'DT': 5986, 'CC': 7355, 'AS': 4118, 'SB': 455, 'DER': 258, 'M': 13790, 'OD': 1675, 'CD': 16182, 'LB': 245, 'PU': 76753, 'VP': 1, 'LC': 7782, 'MSP': 1336, 'JJ': 13234, 'DEV': 634, 'NN': 136643, 'DEC': 12510}\n",
      "\n",
      "标签索引字典内容labels_dict为：\n",
      " {'padded_label': 0, nan: 1, 'VV': 2, 'DEG': 3, 'IJ': 4, 'NR': 5, 'CS': 6, 'SP': 7, 'VA': 8, 'FW': 9, 'PN': 10, 'NT': 11, 'BA': 12, 'P': 13, 'AD': 14, 'ETC': 15, 'X': 16, 'VC': 17, 'VE': 18, 'NP': 19, 'DT': 20, 'CC': 21, 'AS': 22, 'SB': 23, 'DER': 24, 'M': 25, 'OD': 26, 'CD': 27, 'LB': 28, 'PU': 29, 'VP': 30, 'LC': 31, 'MSP': 32, 'JJ': 33, 'DEV': 34, 'NN': 35, 'DEC': 36}\n"
     ]
    }
   ],
   "source": [
    "## 构建标签字典\n",
    "# 合并训练集和测试集的标签\n",
    "labels = y_train.tolist() + y_test.tolist()\n",
    "# 利用集合进行去重，再转化为列表格式，获得标签类型列表\n",
    "labels_types = list(set(labels))\n",
    "print(\"训练集和测试集的总标签类别数len(labels_types)为：\" + str(len(labels_types)))\n",
    "print(\"标签类型列表为：\\n\", labels_types)\n",
    "\n",
    "# 构建标签频次字典（标签：该标签出现次数）\n",
    "labels_dict = {}\n",
    "\n",
    "# 构建标签索引字典（标签： 该标签的索引）\n",
    "# 加入{\"padded_label\" : 0}降低损失率\n",
    "labels_index = {\"padded_label\" : 0}\n",
    "\n",
    "for index in range(len(labels_types)):\n",
    "    # 根据标签类别列表通过索引获取标签\n",
    "    label = labels_types[index]\n",
    "    # 更新标签频次字典\n",
    "    labels_dict.update({label: labels.count(label)}) \n",
    "    # 更新标签索引字典\n",
    "    labels_index.update({label: index+1}) \n",
    "    \n",
    "print(\"\\n训练集和测试集的总标签数len(labels)为：\" + str(len(labels)))\n",
    "\n",
    "# 输出构建好的标签频次字典\n",
    "print(\"\\n标签字典的对象数len(labels_dict)为：\" + str(len(labels_dict)))\n",
    "print(\"标签频次字典内容labels_dict为：\\n\", labels_dict)\n",
    "\n",
    "# 输出构建好的标签索引字典\n",
    "print(\"\\n标签索引字典内容labels_dict为：\\n\", labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中按句进行拆分后的句子为：\n",
      " [array(['上海', '浦东', '开发', '与', '法制', '建设', '同步'], dtype=object), array(['新华社', '上海', '二月', '十日', '电', '（', '记者', '谢金虎', '、', '张持坚', '）'],\n",
      "      dtype=object), array(['上海', '浦东', '近年', '来', '颁布', '实行', '了', '涉及', '经济', '、', '贸易', '、',\n",
      "       '建设', '、', '规划', '、', '科技', '、', '文教', '等', '领域', '的', '七十一', '件',\n",
      "       '法规性', '文件', '，', '确保', '了', '浦东', '开发', '的', '有序', '进行', '。'],\n",
      "      dtype=object), array(['浦东', '开发', '开放', '是', '一', '项', '振兴', '上海', '，', '建设', '现代化',\n",
      "       '经济', '、', '贸易', '、', '金融', '中心', '的', '跨世纪', '工程', '，', '因此',\n",
      "       '大量', '出现', '的', '是', '以前', '不', '曾', '遇到', '过', '的', '新', '情况',\n",
      "       '、', '新', '问题', '。'], dtype=object), array(['对', '此', '，', '浦东', '不', '是', '简单', '的', '采取', '“', '干', '一', '段',\n",
      "       '时间', '，', '等', '积累', '了', '经验', '以后', '再', '制定', '法规', '条例', '”',\n",
      "       '的', '做法', '，', '而', '是', '借鉴', '发达', '国家', '和', '深圳', '等', '特区',\n",
      "       '的', '经验', '教训', '，', '聘请', '国内外', '有关', '专家', '学者', '，', '积极',\n",
      "       '、', '及时', '地', '制定', '和', '推出', '法规性', '文件', '，', '使', '这些', '经济',\n",
      "       '活动', '一', '出现', '就', '被', '纳入', '法制', '轨道', '。'], dtype=object)]\n",
      "\n",
      "训练集中按句进行拆分后的句子所对应的词性为：\n",
      " [array(['NR', 'NR', 'NN', 'CC', 'NN', 'NN', 'VV'], dtype=object), array(['NN', 'NR', 'NT', 'NT', 'NN', 'PU', 'NN', 'NR', 'PU', 'NR', 'PU'],\n",
      "      dtype=object), array(['NR', 'NR', 'NT', 'LC', 'VV', 'VV', 'AS', 'VV', 'NN', 'PU', 'NN',\n",
      "       'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'ETC', 'NN', 'DEC',\n",
      "       'CD', 'M', 'NN', 'NN', 'PU', 'VV', 'AS', 'NR', 'NN', 'DEG', 'JJ',\n",
      "       'NN', 'PU'], dtype=object), array(['NR', 'NN', 'NN', 'VC', 'CD', 'M', 'VV', 'NR', 'PU', 'VV', 'NN',\n",
      "       'NN', 'PU', 'NN', 'PU', 'NN', 'NN', 'DEC', 'JJ', 'NN', 'PU', 'AD',\n",
      "       'AD', 'VV', 'DEC', 'VC', 'NT', 'AD', 'AD', 'VV', 'AS', 'DEC', 'JJ',\n",
      "       'NN', 'PU', 'JJ', 'NN', 'PU'], dtype=object), array(['P', 'PN', 'PU', 'NR', 'AD', 'VC', 'VA', 'DEV', 'VV', 'PU', 'VV',\n",
      "       'CD', 'M', 'NN', 'PU', 'P', 'VV', 'AS', 'NN', 'LC', 'AD', 'VV',\n",
      "       'NN', 'NN', 'PU', 'DEC', 'NN', 'PU', 'CC', 'VC', 'VV', 'JJ', 'NN',\n",
      "       'CC', 'NR', 'ETC', 'NN', 'DEG', 'NN', 'NN', 'PU', 'VV', 'NN', 'JJ',\n",
      "       'NN', 'NN', 'PU', 'AD', 'PU', 'AD', 'DEV', 'VV', 'CC', 'VV', 'NN',\n",
      "       'NN', 'PU', 'VV', 'DT', 'NN', 'NN', 'AD', 'VV', 'AD', 'SB', 'VV',\n",
      "       'NN', 'NN', 'PU'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# 按句子对X、y进行拆分\n",
    "def split_corpus_by_sentence(content):\n",
    "    cleaned_sentence = []\n",
    "    split_label = content.isnull() # 用来判断是否有缺失值，返回布尔值\n",
    "    last_split_index = 0\n",
    "    index = 0\n",
    "    \n",
    "    while index < len(content):\n",
    "        current_word = content[index]\n",
    "        # 如果有缺失值且cleaned_sentence列表中无内容\n",
    "        if split_label[index] == True and len(cleaned_sentence) == 0:\n",
    "            # 添加content列表中从索引last_split_index到index的内容到cleaned_sentence\n",
    "            cleaned_sentence.append(np.array(content[last_split_index: index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "            \n",
    "        # 如果有缺失值且cleaned_sentence列表中有内容\n",
    "        elif split_label[index] == True and len(cleaned_sentence) > 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index: index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "            \n",
    "        else:\n",
    "            index += 1\n",
    "    return cleaned_sentence\n",
    "\n",
    "# 利用上述函数按句子拆分训练集和测试集的文本和标签\n",
    "X_train_sent_split = split_corpus_by_sentence(X_train)\n",
    "y_train_sent_split = split_corpus_by_sentence(y_train)\n",
    "X_test_sent_split = split_corpus_by_sentence(X_test)\n",
    "y_test_sent_split = split_corpus_by_sentence(y_test)\n",
    "\n",
    "print('训练集中按句进行拆分后的句子为：\\n', X_train_sent_split[:5])\n",
    "print('\\n训练集中按句进行拆分后的句子所对应的词性为：\\n', y_train_sent_split[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中的标签转换为索引对应为：\n",
      " [[4, 4, 34, 20, 34, 34, 1], [34, 4, 10, 10, 34, 28, 34, 4, 28, 4, 28], [4, 4, 10, 30, 1, 1, 21, 1, 34, 28, 34, 28, 34, 28, 34, 28, 34, 28, 34, 14, 34, 35, 26, 24, 34, 34, 28, 1, 21, 4, 34, 2, 32, 34, 28], [4, 34, 34, 16, 26, 24, 1, 4, 28, 1, 34, 34, 28, 34, 28, 34, 34, 35, 32, 34, 28, 13, 13, 1, 35, 16, 10, 13, 13, 1, 21, 35, 32, 34, 28, 32, 34, 28], [12, 9, 28, 4, 13, 16, 7, 33, 1, 28, 1, 26, 24, 34, 28, 12, 1, 21, 34, 30, 13, 1, 34, 34, 28, 35, 34, 28, 20, 16, 1, 32, 34, 20, 4, 14, 34, 2, 34, 34, 28, 1, 34, 32, 34, 34, 28, 13, 28, 13, 33, 1, 20, 1, 34, 34, 28, 1, 19, 34, 34, 13, 1, 13, 22, 1, 34, 34, 28]]\n",
      "\n",
      "测试集中的标签转换为索引对应为：\n",
      " [[4, 4, 12, 4, 1, 32, 34, 34, 34], [4, 4, 10, 10, 34, 28, 34, 4, 28, 4, 28], [28, 4, 4, 34, 32, 34, 34, 34, 34, 28, 10, 12, 4, 1, 28], [10, 12, 9, 1, 35, 16, 34, 34, 34, 34, 34, 20, 34, 34, 34, 26, 24, 34, 28, 13, 13, 1, 21, 34, 34, 34, 34, 34, 28], [19, 26, 24, 34, 16, 13, 12, 34, 34, 34, 34, 34, 34, 34, 34, 4, 34, 20, 4, 4, 32, 34, 34, 28, 12, 4, 34, 34, 34, 34, 28, 34, 34, 34, 28, 4, 34, 1, 35, 28]]\n"
     ]
    }
   ],
   "source": [
    "# 根据之前构建的标签字典将标签文本转换为索引\n",
    "def transfer_label_category_index(origin_labels, labels_types):\n",
    "    transfered_label = []\n",
    "    for sentence_labels in origin_labels:\n",
    "        # 将标签依据字典转换为序号\n",
    "        labels_format_index = [labels_types.index(label) for label in sentence_labels]\n",
    "        transfered_label.append(labels_format_index)\n",
    "    return transfered_label\n",
    "\n",
    "# 利用上述函数将训练集和测试集的标签转换为索引\n",
    "y_train_index = transfer_label_category_index(y_train_sent_split, labels_types)\n",
    "y_test_index = transfer_label_category_index(y_test_sent_split, labels_types)\n",
    "\n",
    "print('训练集中的标签转换为索引对应为：\\n', y_train_index[:5])\n",
    "print('\\n测试集中的标签转换为索引对应为：\\n', y_test_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中填充的张量为：\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 1. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "测试集中填充的张量为：\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 设置句子的最大长度为100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "## 进行标签格式转化\n",
    "# 利用zeros(shape, dtype=float, order='C')构建张量，值全为0\n",
    "# 形状shape对应（标签样本数，句子长度，标签类别数）\n",
    "y_train_index_padded = np.zeros((len(y_train_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), # 形状 \n",
    "                                dtype='float', # 数据类型 \n",
    "                                order='C',     # c代表与c语言类似，行优先\n",
    "                               )\n",
    "y_test_index_padded = np.zeros((len(y_test_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), # 形状 \n",
    "                              dtype='float', # 数据类型 \n",
    "                              order='C',     # c代表与c语言类似，行优先\n",
    "                              )\n",
    "\n",
    "''' \n",
    "填充张量\n",
    "嵌套循环遍历：先句子后词\n",
    "'''\n",
    "# 训练集\n",
    "for sentence_labels_index in range(len(y_train_index)):\n",
    "    for label_index in range(len(y_train_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_train_index_padded[sentence_labels_index, label_index, y_train_index[sentence_labels_index][label_index]+1] = 1\n",
    "    # 优化：若为填充的标签，则将其预测为第一位为1        \n",
    "    if len(y_train_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_train_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_train_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "\n",
    "# 测试集      \n",
    "for sentence_labels_index in range(len(y_test_index)):\n",
    "    for label_index in range(len(y_test_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_test_index_padded[sentence_labels_index, label_index, y_test_index[sentence_labels_index][label_index]+1] = 1        \n",
    "    # 优化：若为填充的标签，则将其预测为第一位为1 \n",
    "    if len(y_test_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_test_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_test_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "    \n",
    "print('训练集中填充的张量为：\\n', y_train_index_padded[:1])\n",
    "print('\\n测试集中填充的张量为：\\n', y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. word2vec模型导入\n",
    "预训练的word2vec模型采用前人用中文维基百科训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "'''（1）导入 预训练的词向量'''\n",
    "\n",
    "# 本地词向量的地址\n",
    "myPath = './data/sgns.wiki.word'   \n",
    "\n",
    "# 以二进制读取词向量\n",
    "Word2VecModel = gensim.models.KeyedVectors.load_word2vec_format(myPath).wv\n",
    "\n",
    "# 词语的向量，是numpy格式\n",
    "vector = Word2VecModel.wv['空间']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的类型为： <class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "，\n",
      "Vocab(count:352217, index:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 输出词向量的类型\n",
    "print('词向量的类型为：', type(Word2VecModel.wv))  \n",
    "# 结果为：Word2VecKeyedVectors\n",
    "\n",
    "# 获取word2vec训练后model中的所有的词\n",
    "for i, j in Word2VecModel.wv.vocab.items():\n",
    "    # 此时 i 代表每个单词\n",
    "    print(i) \n",
    "    \n",
    "    # j 代表封装了 词频 等信息的 gensim“Vocab”对象\n",
    "    # 例子：Vocab(count:1481, index:38, sample_int:3701260191)\n",
    "    print(j)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "'''（2）构造包含所有词语的list；\n",
    "        初始化“词语-序号”字典和“词向量”矩阵\n",
    "'''\n",
    "# 存储所有的单词和gensim“Vocab”对象\n",
    "vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]\n",
    "\n",
    "# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "word_index = {\" \": 0}\n",
    "\n",
    "# 初始化`[word : vector]`字典\n",
    "word_vector = {}\n",
    "\n",
    "# 初始化 存储所有向量的大矩阵【其中多一位（首行）：词向量全为 0，用于 padding补零。\n",
    "# 行数为：所有单词数+1。比如 10000+1 ； \n",
    "# 列数为：词向量“维度”。比如100。\n",
    "embeddings_matrix = np.zeros((len(vocab_list)+1, Word2VecModel.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "'''（3）填充上述的字典word_index和大矩阵embeddings_matrix'''\n",
    "\n",
    "# 循环遍历 存储所有的单词和gensim“Vocab”对象的 vocab_list列表\n",
    "for i in range(len(vocab_list)):\n",
    "    #print(i)    \n",
    "    # 将每个词语存储到vocab_list列表中\n",
    "    word = vocab_list[i]    \n",
    "    # 将每个词语及对应的序号存储到word_index字典中\n",
    "    word_index[word] = i + 1   \n",
    "    # 将每个{词语：词向量}存储到word_vector字典中\n",
    "    word_vector[word] = Word2VecModel.wv[word]   \n",
    "    # 将每个{词向量：序号+1}存储到词向量矩阵embeddings_matrix中\n",
    "    embeddings_matrix[i + 1] = Word2VecModel.wv[word]\n",
    "    \n",
    "# np.save('x_word_index.npy', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中的词语转换为索引为：\n",
      " [[  347 16980   507    10 15537   603  4380     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "测试集中的词语转换为索引为：\n",
      " [[  28  523    6 5705 2208  287 1216  587 1523    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "'''（4）序号化 文本；\n",
    "        tokenizer句子；\n",
    "        并返回每个句子所对应的词语索引\n",
    "'''\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# 由于将词语转化为索引的word_index需要与词向量模型对齐，故在导入词向量模型后再将X进行处理\n",
    "def tokenizer(texts, word_index):\n",
    "    data = []\n",
    "    \n",
    "    for sentence in texts:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                # 根据word_index字典把文本中的词语转化为index\n",
    "                new_sentence.append(word_index[word])\n",
    "            except:\n",
    "                new_sentence.append(0)\n",
    "        data.append(new_sentence)\n",
    "        \n",
    "    # 使用kears的内置函数padding对齐句子（好处是输出numpy数组，不用自己转化了）\n",
    "    data = sequence.pad_sequences(data,\n",
    "                                 maxlen = MAX_SEQUENCE_LENGTH, # 序列的最大长度（大于此长度的序列将被截短，小于此长度的序列将在后部填0）\n",
    "                                 padding = 'post', # 当需要补0时，在序列的结尾补\n",
    "                                 truncating = 'post', # 当需要截断序列时，从结尾截断\n",
    "                                 )\n",
    "    return data\n",
    "\n",
    "# 将训练集中的词语转换为索引\n",
    "X_train_tokenized = tokenizer(X_train_sent_split, word_index)\n",
    "\n",
    "# 将测试集中的词语转换为索引\n",
    "X_test_tokenized = tokenizer(X_test_sent_split, word_index)\n",
    "\n",
    "print('训练集中的词语转换为索引为：\\n', X_train_tokenized[:1])\n",
    "print('\\n测试集中的词语转换为索引为：\\n', X_test_tokenized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 标引网络构建及训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import keras\n",
    "from keras import optimizers\n",
    "\n",
    "# 词向量维度为300\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量长度（300）\n",
    "                    weights = [embeddings_matrix], # 预训练的词向量系数\n",
    "                    input_length = MAX_SEQUENCE_LENGTH, # 每句话的最大长度（必须padding） \n",
    "                    trainable = False, # 是否在训练的过程中更新词向量\n",
    "                   ),\n",
    "         )\n",
    "\n",
    "## input_shape=(Batch_size, Time_step, Input_Sizes)\n",
    "# 输入层\n",
    "# 可以增加bidirectional改变效果\n",
    "model.add(LSTM(128, \n",
    "               input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "               activation='tanh', # 激活函数\n",
    "               return_sequences=True, # 返回全部time step的hidden state值\n",
    "              )\n",
    "         )\n",
    "model.add(Dropout(0.5)) # 减少过拟合\n",
    "\n",
    "# 隐藏层\n",
    "model.add(Dense(64, \n",
    "                input_shape=(MAX_SEQUENCE_LENGTH, 128), # 128维\n",
    "                activation='relu',\n",
    "               )\n",
    "         )\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 输出层\n",
    "model.add(Dense(len(labels_types)+1,\n",
    "                input_shape=(MAX_SEQUENCE_LENGTH, 64),  # 64维\n",
    "                activation='softmax',\n",
    "               )\n",
    "         )\n",
    "\n",
    "# 设置SGD的参数\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', # 损失函数\n",
    "              optimizer=sgd, # 优化器采用SGD\n",
    "              metrics=['accuracy'], # 评价指标是准确率\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          105665400 \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100, 64)           8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 37)           2405      \n",
      "=================================================================\n",
      "Total params: 105,895,709\n",
      "Trainable params: 230,309\n",
      "Non-trainable params: 105,665,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 输出模型各层的参数状况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中将词语转换为索引后: (18078, 100)\n",
      "\n",
      "训练集中将标签转换为索引并填充张量后 (18078, 100, 37)\n"
     ]
    }
   ],
   "source": [
    "# 输出训练集中将词语转换为索引后的shape\n",
    "print('训练集中将词语转换为索引后:', X_train_tokenized.shape)\n",
    "# 18078个样本,每个样本100个词\n",
    "\n",
    "# 输出训练集中将标签转换为索引并填充张量后的shape\n",
    "print('\\n训练集中将标签转换为索引并填充张量后', y_train_index_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 43s 303ms/step - loss: 1.2304 - accuracy: 0.7543\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 42s 293ms/step - loss: 0.7655 - accuracy: 0.7920\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 42s 296ms/step - loss: 0.7244 - accuracy: 0.8037\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 42s 298ms/step - loss: 0.6942 - accuracy: 0.8131\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 42s 293ms/step - loss: 0.6669 - accuracy: 0.8205\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 41s 290ms/step - loss: 0.6406 - accuracy: 0.8261\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 42s 297ms/step - loss: 0.6171 - accuracy: 0.8300\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 41s 292ms/step - loss: 0.5955 - accuracy: 0.8334\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 42s 295ms/step - loss: 0.5757 - accuracy: 0.8367\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 41s 290ms/step - loss: 0.5576 - accuracy: 0.8403\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 41s 291ms/step - loss: 0.5407 - accuracy: 0.8438\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 43s 304ms/step - loss: 0.5244 - accuracy: 0.8478\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 41s 292ms/step - loss: 0.5087 - accuracy: 0.8521\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 42s 295ms/step - loss: 0.4942 - accuracy: 0.8563\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 41s 292ms/step - loss: 0.4800 - accuracy: 0.8605\n",
      "Epoch 16/20\n",
      "142/142 [==============================] - 42s 293ms/step - loss: 0.4663 - accuracy: 0.8646\n",
      "Epoch 17/20\n",
      "142/142 [==============================] - 42s 292ms/step - loss: 0.4535 - accuracy: 0.8682\n",
      "Epoch 18/20\n",
      "142/142 [==============================] - 41s 291ms/step - loss: 0.4413 - accuracy: 0.8714\n",
      "Epoch 19/20\n",
      "142/142 [==============================] - 41s 291ms/step - loss: 0.4299 - accuracy: 0.8745\n",
      "Epoch 20/20\n",
      "142/142 [==============================] - 41s 291ms/step - loss: 0.4188 - accuracy: 0.8775\n",
      "-------------------Evaluation------------------------\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.2863 - accuracy: 0.9119\n"
     ]
    }
   ],
   "source": [
    "# 使用训练集对模型进行训练\n",
    "model.fit(X_train_tokenized,\n",
    "          y_train_index_padded,\n",
    "          epochs = 20,  # 训练20轮\n",
    "          batch_size = 128, # 指定进行梯度下降时每个batch包含的样本数为128\n",
    "          verbose = 1, # 展示训练过程\n",
    "         )\n",
    "\n",
    "# 评估训练完成的模型.返回损失值&模型的度量值.\n",
    "print('-------------------Evaluation------------------------')\n",
    "score = model.evaluate(X_test_tokenized, \n",
    "                       y_test_index_padded,\n",
    "                       batch_size=128,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存optim_sgd模型\n",
    "model.save('cn_pos_tag_optim_sgd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练的模型经在测试集上验证获得的loss和accuracy为：\n",
      "[0.2862928509712219, 0.9119253158569336]\n"
     ]
    }
   ],
   "source": [
    "print('训练的模型经在测试集上验证获得的loss和accuracy为：')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------对测试集进行预测------------------------\n",
      "（1）对测试集 X_test_tokenized[:1] 进行预测：\n",
      "[[[6.9098040e-03 6.2825326e-03 7.8433312e-02 ... 6.8781371e-03\n",
      "   2.8639570e-02 3.2510314e-02]\n",
      "  [1.1138821e-03 9.2179846e-04 9.3083501e-02 ... 1.4266926e-03\n",
      "   2.9386207e-01 1.6350476e-02]\n",
      "  [1.2290626e-03 2.2244744e-03 3.4066811e-02 ... 2.8237212e-03\n",
      "   1.6270354e-02 6.4825661e-02]\n",
      "  ...\n",
      "  [9.9999988e-01 1.6198903e-12 3.6877609e-11 ... 1.9128993e-12\n",
      "   5.8081528e-10 4.0998216e-12]\n",
      "  [9.9999988e-01 1.6192694e-12 3.6864387e-11 ... 1.9121662e-12\n",
      "   5.8063254e-10 4.0982426e-12]\n",
      "  [9.9999988e-01 1.6187105e-12 3.6852573e-11 ... 1.9115098e-12\n",
      "   5.8046867e-10 4.0968279e-12]]]\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行预测\n",
    "print('-----------------------对测试集进行预测------------------------')\n",
    "print('（1）对测试集 X_test_tokenized[:1] 进行预测：')\n",
    "print(model.predict(X_test_tokenized[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中标签填充的张量 y_test_index_padded[:1] 为：\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 输出测试集中标签填充的张量\n",
    "print('测试集中标签填充的张量 y_test_index_padded[:1] 为：')\n",
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（2）对测试集 X_test_tokenized[2: 3] 进行预测：\n",
      "[[[4.6644676e-03 3.7212964e-04 2.0739450e-03 ... 4.1722335e-04\n",
      "   8.4141770e-04 6.7957370e-03]\n",
      "  [4.8672273e-03 5.6342296e-03 6.7360811e-02 ... 6.0775043e-03\n",
      "   2.3814259e-02 4.1300923e-02]\n",
      "  [8.0049201e-04 6.8240636e-04 8.4046304e-02 ... 1.1106557e-03\n",
      "   3.2326984e-01 1.5300987e-02]\n",
      "  ...\n",
      "  [9.9999988e-01 1.6217173e-12 3.6916383e-11 ... 1.9150460e-12\n",
      "   5.8135396e-10 4.1044616e-12]\n",
      "  [9.9999988e-01 1.6209164e-12 3.6899418e-11 ... 1.9141075e-12\n",
      "   5.8111782e-10 4.1024345e-12]\n",
      "  [9.9999988e-01 1.6202085e-12 3.6884430e-11 ... 1.9132716e-12\n",
      "   5.8091060e-10 4.1006352e-12]]]\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行预测\n",
    "print('（2）对测试集 X_test_tokenized[2: 3] 进行预测：')\n",
    "print(model.predict(X_test_tokenized[2: 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中标签填充的张量 y_test_index_padded[:1] 为：\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 输出测试集中标签填充的张量\n",
    "print('测试集中标签填充的张量 y_test_index_padded[:1] 为：')\n",
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
