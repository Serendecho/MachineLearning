{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习进行词性标注（使用RNN和BRNN模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# 训练集和测试集的地址\n",
    "train_data = './data/ctb5.1-pos/train.tsv'\n",
    "test_data = './data/ctb5.1-pos/test.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据读取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集，并提取出其中的文本部分和标签部分\n",
    "def get_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t', \n",
    "                       skip_blank_lines=False, \n",
    "                       header=None,\n",
    "                      )\n",
    "    \n",
    "    # 取出文本部分\n",
    "    content = data[0]\n",
    "    \n",
    "    # 取出标签部分\n",
    "    label = data[1]\n",
    "    \n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用get_data函数获取训练集和测试集的文本和标签\n",
    "X_train, y_train = get_data(train_data)\n",
    "X_test, y_test = get_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据预处理与格式转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集和测试集的总标签类别数len(labels_types)为：36\n",
      "标签类型列表为：\n",
      " [nan, 'PU', 'NN', 'CC', 'NP', 'NT', 'DEG', 'CD', 'DT', 'LB', 'BA', 'IJ', 'JJ', 'MSP', 'VP', 'FW', 'PN', 'VA', 'CS', 'NR', 'X', 'DEC', 'M', 'AD', 'DER', 'VE', 'AS', 'VV', 'VC', 'DEV', 'P', 'SB', 'OD', 'LC', 'ETC', 'SP']\n",
      "\n",
      "训练集和测试集的总标签数len(labels)为：520125\n",
      "\n",
      "标签字典的对象数len(labels_dict)为：36\n",
      "标签频次字典内容labels_dict为：\n",
      " {nan: 18426, 'PU': 76753, 'NN': 136643, 'CC': 7355, 'NP': 5, 'NT': 9659, 'DEG': 12337, 'CD': 16182, 'DT': 5986, 'LB': 245, 'BA': 755, 'IJ': 12, 'JJ': 13234, 'MSP': 1336, 'VP': 1, 'FW': 33, 'PN': 6644, 'VA': 7755, 'CS': 892, 'NR': 30570, 'X': 6, 'DEC': 12510, 'M': 13790, 'AD': 36430, 'DER': 258, 'VE': 3005, 'AS': 4118, 'VV': 69858, 'VC': 5404, 'DEV': 634, 'P': 17606, 'SB': 455, 'OD': 1675, 'LC': 7782, 'ETC': 1303, 'SP': 468}\n",
      "\n",
      "标签索引字典内容labels_dict为：\n",
      " {'padded_label': 0, nan: 1, 'PU': 2, 'NN': 3, 'CC': 4, 'NP': 5, 'NT': 6, 'DEG': 7, 'CD': 8, 'DT': 9, 'LB': 10, 'BA': 11, 'IJ': 12, 'JJ': 13, 'MSP': 14, 'VP': 15, 'FW': 16, 'PN': 17, 'VA': 18, 'CS': 19, 'NR': 20, 'X': 21, 'DEC': 22, 'M': 23, 'AD': 24, 'DER': 25, 'VE': 26, 'AS': 27, 'VV': 28, 'VC': 29, 'DEV': 30, 'P': 31, 'SB': 32, 'OD': 33, 'LC': 34, 'ETC': 35, 'SP': 36}\n"
     ]
    }
   ],
   "source": [
    "## 构建标签字典\n",
    "# 合并训练集和测试集的标签\n",
    "labels = y_train.tolist() + y_test.tolist()\n",
    "# 利用集合进行去重，再转化为列表格式，获得标签类型列表\n",
    "labels_types = list(set(labels))\n",
    "print(\"训练集和测试集的总标签类别数len(labels_types)为：\" + str(len(labels_types)))\n",
    "print(\"标签类型列表为：\\n\", labels_types)\n",
    "\n",
    "# 构建标签频次字典（标签：该标签出现次数）\n",
    "labels_dict = {}\n",
    "\n",
    "# 构建标签索引字典（标签： 该标签的索引）\n",
    "# 加入{\"padded_label\" : 0}降低损失率\n",
    "labels_index = {\"padded_label\" : 0}\n",
    "\n",
    "for index in range(len(labels_types)):\n",
    "    # 根据标签类别列表通过索引获取标签\n",
    "    label = labels_types[index]\n",
    "    # 更新标签频次字典\n",
    "    labels_dict.update({label: labels.count(label)}) \n",
    "    # 更新标签索引字典\n",
    "    labels_index.update({label: index+1}) \n",
    "    \n",
    "print(\"\\n训练集和测试集的总标签数len(labels)为：\" + str(len(labels)))\n",
    "\n",
    "# 输出构建好的标签频次字典\n",
    "print(\"\\n标签字典的对象数len(labels_dict)为：\" + str(len(labels_dict)))\n",
    "print(\"标签频次字典内容labels_dict为：\\n\", labels_dict)\n",
    "\n",
    "# 输出构建好的标签索引字典\n",
    "print(\"\\n标签索引字典内容labels_dict为：\\n\", labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中按句进行拆分后的句子为：\n",
      " [array(['上海', '浦东', '开发', '与', '法制', '建设', '同步'], dtype=object), array(['新华社', '上海', '二月', '十日', '电', '（', '记者', '谢金虎', '、', '张持坚', '）'],\n",
      "      dtype=object), array(['上海', '浦东', '近年', '来', '颁布', '实行', '了', '涉及', '经济', '、', '贸易', '、',\n",
      "       '建设', '、', '规划', '、', '科技', '、', '文教', '等', '领域', '的', '七十一', '件',\n",
      "       '法规性', '文件', '，', '确保', '了', '浦东', '开发', '的', '有序', '进行', '。'],\n",
      "      dtype=object), array(['浦东', '开发', '开放', '是', '一', '项', '振兴', '上海', '，', '建设', '现代化',\n",
      "       '经济', '、', '贸易', '、', '金融', '中心', '的', '跨世纪', '工程', '，', '因此',\n",
      "       '大量', '出现', '的', '是', '以前', '不', '曾', '遇到', '过', '的', '新', '情况',\n",
      "       '、', '新', '问题', '。'], dtype=object), array(['对', '此', '，', '浦东', '不', '是', '简单', '的', '采取', '“', '干', '一', '段',\n",
      "       '时间', '，', '等', '积累', '了', '经验', '以后', '再', '制定', '法规', '条例', '”',\n",
      "       '的', '做法', '，', '而', '是', '借鉴', '发达', '国家', '和', '深圳', '等', '特区',\n",
      "       '的', '经验', '教训', '，', '聘请', '国内外', '有关', '专家', '学者', '，', '积极',\n",
      "       '、', '及时', '地', '制定', '和', '推出', '法规性', '文件', '，', '使', '这些', '经济',\n",
      "       '活动', '一', '出现', '就', '被', '纳入', '法制', '轨道', '。'], dtype=object)]\n",
      "\n",
      "训练集中按句进行拆分后的句子所对应的词性为：\n",
      " [array(['NR', 'NR', 'NN', 'CC', 'NN', 'NN', 'VV'], dtype=object), array(['NN', 'NR', 'NT', 'NT', 'NN', 'PU', 'NN', 'NR', 'PU', 'NR', 'PU'],\n",
      "      dtype=object), array(['NR', 'NR', 'NT', 'LC', 'VV', 'VV', 'AS', 'VV', 'NN', 'PU', 'NN',\n",
      "       'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'ETC', 'NN', 'DEC',\n",
      "       'CD', 'M', 'NN', 'NN', 'PU', 'VV', 'AS', 'NR', 'NN', 'DEG', 'JJ',\n",
      "       'NN', 'PU'], dtype=object), array(['NR', 'NN', 'NN', 'VC', 'CD', 'M', 'VV', 'NR', 'PU', 'VV', 'NN',\n",
      "       'NN', 'PU', 'NN', 'PU', 'NN', 'NN', 'DEC', 'JJ', 'NN', 'PU', 'AD',\n",
      "       'AD', 'VV', 'DEC', 'VC', 'NT', 'AD', 'AD', 'VV', 'AS', 'DEC', 'JJ',\n",
      "       'NN', 'PU', 'JJ', 'NN', 'PU'], dtype=object), array(['P', 'PN', 'PU', 'NR', 'AD', 'VC', 'VA', 'DEV', 'VV', 'PU', 'VV',\n",
      "       'CD', 'M', 'NN', 'PU', 'P', 'VV', 'AS', 'NN', 'LC', 'AD', 'VV',\n",
      "       'NN', 'NN', 'PU', 'DEC', 'NN', 'PU', 'CC', 'VC', 'VV', 'JJ', 'NN',\n",
      "       'CC', 'NR', 'ETC', 'NN', 'DEG', 'NN', 'NN', 'PU', 'VV', 'NN', 'JJ',\n",
      "       'NN', 'NN', 'PU', 'AD', 'PU', 'AD', 'DEV', 'VV', 'CC', 'VV', 'NN',\n",
      "       'NN', 'PU', 'VV', 'DT', 'NN', 'NN', 'AD', 'VV', 'AD', 'SB', 'VV',\n",
      "       'NN', 'NN', 'PU'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# 按句子对X、y进行拆分\n",
    "def split_corpus_by_sentence(content):\n",
    "    cleaned_sentence = []\n",
    "    split_label = content.isnull() # 用来判断是否有缺失值，返回布尔值\n",
    "    last_split_index = 0\n",
    "    index = 0\n",
    "    \n",
    "    while index < len(content):\n",
    "        current_word = content[index]\n",
    "        # 如果有缺失值且cleaned_sentence列表中无内容\n",
    "        if split_label[index] == True and len(cleaned_sentence) == 0:\n",
    "            # 添加content列表中从索引last_split_index到index的内容到cleaned_sentence\n",
    "            cleaned_sentence.append(np.array(content[last_split_index: index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "            \n",
    "        # 如果有缺失值且cleaned_sentence列表中有内容\n",
    "        elif split_label[index] == True and len(cleaned_sentence) > 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index: index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "            \n",
    "        else:\n",
    "            index += 1\n",
    "    return cleaned_sentence\n",
    "\n",
    "# 利用上述函数按句子拆分训练集和测试集的文本和标签\n",
    "X_train_sent_split = split_corpus_by_sentence(X_train)\n",
    "y_train_sent_split = split_corpus_by_sentence(y_train)\n",
    "X_test_sent_split = split_corpus_by_sentence(X_test)\n",
    "y_test_sent_split = split_corpus_by_sentence(y_test)\n",
    "\n",
    "print('训练集中按句进行拆分后的句子为：\\n', X_train_sent_split[:5])\n",
    "print('\\n训练集中按句进行拆分后的句子所对应的词性为：\\n', y_train_sent_split[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中的标签转换为索引对应为：\n",
      " [[19, 19, 2, 3, 2, 2, 27], [2, 19, 5, 5, 2, 1, 2, 19, 1, 19, 1], [19, 19, 5, 33, 27, 27, 26, 27, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 34, 2, 21, 7, 22, 2, 2, 1, 27, 26, 19, 2, 6, 12, 2, 1], [19, 2, 2, 28, 7, 22, 27, 19, 1, 27, 2, 2, 1, 2, 1, 2, 2, 21, 12, 2, 1, 23, 23, 27, 21, 28, 5, 23, 23, 27, 26, 21, 12, 2, 1, 12, 2, 1], [30, 16, 1, 19, 23, 28, 17, 29, 27, 1, 27, 7, 22, 2, 1, 30, 27, 26, 2, 33, 23, 27, 2, 2, 1, 21, 2, 1, 3, 28, 27, 12, 2, 3, 19, 34, 2, 6, 2, 2, 1, 27, 2, 12, 2, 2, 1, 23, 1, 23, 29, 27, 3, 27, 2, 2, 1, 27, 8, 2, 2, 23, 27, 23, 31, 27, 2, 2, 1]]\n",
      "\n",
      "测试集中的标签转换为索引对应为：\n",
      " [[19, 19, 30, 19, 27, 12, 2, 2, 2], [19, 19, 5, 5, 2, 1, 2, 19, 1, 19, 1], [1, 19, 19, 2, 12, 2, 2, 2, 2, 1, 5, 30, 19, 27, 1], [5, 30, 16, 27, 21, 28, 2, 2, 2, 2, 2, 3, 2, 2, 2, 7, 22, 2, 1, 23, 23, 27, 26, 2, 2, 2, 2, 2, 1], [8, 7, 22, 2, 28, 23, 30, 2, 2, 2, 2, 2, 2, 2, 2, 19, 2, 3, 19, 19, 12, 2, 2, 1, 30, 19, 2, 2, 2, 2, 1, 2, 2, 2, 1, 19, 2, 27, 21, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 根据之前构建的标签字典将标签文本转换为索引\n",
    "def transfer_label_category_index(origin_labels, labels_types):\n",
    "    transfered_label = []\n",
    "    for sentence_labels in origin_labels:\n",
    "        # 将标签依据字典转换为序号\n",
    "        labels_format_index = [labels_types.index(label) for label in sentence_labels]\n",
    "        transfered_label.append(labels_format_index)\n",
    "    return transfered_label\n",
    "\n",
    "# 利用上述函数将训练集和测试集的标签转换为索引\n",
    "y_train_index = transfer_label_category_index(y_train_sent_split, labels_types)\n",
    "y_test_index = transfer_label_category_index(y_test_sent_split, labels_types)\n",
    "\n",
    "print('训练集中的标签转换为索引对应为：\\n', y_train_index[:5])\n",
    "print('\\n测试集中的标签转换为索引对应为：\\n', y_test_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中填充的张量为：\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "测试集中填充的张量为：\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 设置句子的最大长度为100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "## 进行标签格式转化\n",
    "# 利用zeros(shape, dtype=float, order='C')构建张量，值全为0\n",
    "# 形状shape对应（标签样本数，句子长度，标签类别数）\n",
    "y_train_index_padded = np.zeros((len(y_train_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), # 形状 \n",
    "                                dtype='float', # 数据类型 \n",
    "                                order='C',     # c代表与c语言类似，行优先\n",
    "                               )\n",
    "y_test_index_padded = np.zeros((len(y_test_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), # 形状 \n",
    "                              dtype='float', # 数据类型 \n",
    "                              order='C',     # c代表与c语言类似，行优先\n",
    "                              )\n",
    "\n",
    "''' \n",
    "填充张量\n",
    "嵌套循环遍历：先句子后词\n",
    "'''\n",
    "# 训练集\n",
    "for sentence_labels_index in range(len(y_train_index)):\n",
    "    for label_index in range(len(y_train_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_train_index_padded[sentence_labels_index, label_index, y_train_index[sentence_labels_index][label_index]+1] = 1\n",
    "    # 优化：若为填充的标签，则将其预测为第一位为1        \n",
    "    if len(y_train_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_train_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_train_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "\n",
    "# 测试集      \n",
    "for sentence_labels_index in range(len(y_test_index)):\n",
    "    for label_index in range(len(y_test_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_test_index_padded[sentence_labels_index, label_index, y_test_index[sentence_labels_index][label_index]+1] = 1        \n",
    "    # 优化：若为填充的标签，则将其预测为第一位为1 \n",
    "    if len(y_test_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_test_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_test_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "    \n",
    "print('训练集中填充的张量为：\\n', y_train_index_padded[:1])\n",
    "print('\\n测试集中填充的张量为：\\n', y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. word2vec模型导入\n",
    "预训练的word2vec模型采用前人用中文维基百科训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "'''（1）导入 预训练的词向量'''\n",
    "\n",
    "# 本地词向量的地址\n",
    "myPath = './data/sgns.wiki.word'   \n",
    "\n",
    "# 以二进制读取词向量\n",
    "Word2VecModel = gensim.models.KeyedVectors.load_word2vec_format(myPath).wv\n",
    "\n",
    "# 词语的向量，是numpy格式\n",
    "vector = Word2VecModel.wv['空间']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的类型为： <class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "，\n",
      "Vocab(count:352217, index:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 输出词向量的类型\n",
    "print('词向量的类型为：', type(Word2VecModel.wv))  \n",
    "# 结果为：Word2VecKeyedVectors\n",
    "\n",
    "# 获取word2vec训练后model中的所有的词\n",
    "for i, j in Word2VecModel.wv.vocab.items():\n",
    "    # 此时 i 代表每个单词\n",
    "    print(i) \n",
    "    \n",
    "    # j 代表封装了 词频 等信息的 gensim“Vocab”对象\n",
    "    # 例子：Vocab(count:1481, index:38, sample_int:3701260191)\n",
    "    print(j)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "'''（2）构造包含所有词语的list；\n",
    "        初始化“词语-序号”字典和“词向量”矩阵\n",
    "'''\n",
    "# 存储所有的单词和gensim“Vocab”对象\n",
    "vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]\n",
    "\n",
    "# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "word_index = {\" \": 0}\n",
    "\n",
    "# 初始化`[word : vector]`字典\n",
    "word_vector = {}\n",
    "\n",
    "# 初始化 存储所有向量的大矩阵【其中多一位（首行）：词向量全为 0，用于 padding补零。\n",
    "# 行数为：所有单词数+1。比如 10000+1 ； \n",
    "# 列数为：词向量“维度”。比如100。\n",
    "embeddings_matrix = np.zeros((len(vocab_list)+1, Word2VecModel.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\Anaconda3-5.3.0\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "'''（3）填充上述的字典word_index和大矩阵embeddings_matrix'''\n",
    "\n",
    "# 循环遍历 存储所有的单词和gensim“Vocab”对象的 vocab_list列表\n",
    "for i in range(len(vocab_list)):\n",
    "    #print(i)    \n",
    "    # 将每个词语存储到vocab_list列表中\n",
    "    word = vocab_list[i]    \n",
    "    # 将每个词语及对应的序号存储到word_index字典中\n",
    "    word_index[word] = i + 1   \n",
    "    # 将每个{词语：词向量}存储到word_vector字典中\n",
    "    word_vector[word] = Word2VecModel.wv[word]   \n",
    "    # 将每个{词向量：序号+1}存储到词向量矩阵embeddings_matrix中\n",
    "    embeddings_matrix[i + 1] = Word2VecModel.wv[word]\n",
    "    \n",
    "# np.save('x_word_index.npy', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中的词语转换为索引为：\n",
      " [[  347 16980   507    10 15537   603  4380     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "测试集中的词语转换为索引为：\n",
      " [[  28  523    6 5705 2208  287 1216  587 1523    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "'''（4）序号化 文本；\n",
    "        tokenizer句子；\n",
    "        并返回每个句子所对应的词语索引\n",
    "'''\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# 由于将词语转化为索引的word_index需要与词向量模型对齐，故在导入词向量模型后再将X进行处理\n",
    "def tokenizer(texts, word_index):\n",
    "    data = []\n",
    "    \n",
    "    for sentence in texts:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                # 根据word_index字典把文本中的词语转化为index\n",
    "                new_sentence.append(word_index[word])\n",
    "            except:\n",
    "                new_sentence.append(0)\n",
    "        data.append(new_sentence)\n",
    "        \n",
    "    # 使用kears的内置函数padding对齐句子（好处是输出numpy数组，不用自己转化了）\n",
    "    data = sequence.pad_sequences(data,\n",
    "                                 maxlen = MAX_SEQUENCE_LENGTH, # 序列的最大长度（大于此长度的序列将被截短，小于此长度的序列将在后部填0）\n",
    "                                 padding = 'post', # 当需要补0时，在序列的结尾补\n",
    "                                 truncating = 'post', # 当需要截断序列时，从结尾截断\n",
    "                                 )\n",
    "    return data\n",
    "\n",
    "# 将训练集中的词语转换为索引\n",
    "X_train_tokenized = tokenizer(X_train_sent_split, word_index)\n",
    "\n",
    "# 将测试集中的词语转换为索引\n",
    "X_test_tokenized = tokenizer(X_test_sent_split, word_index)\n",
    "\n",
    "print('训练集中的词语转换为索引为：\\n', X_train_tokenized[:1])\n",
    "print('\\n测试集中的词语转换为索引为：\\n', X_test_tokenized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 标引网络构建及训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Bidirectional\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "import keras\n",
    "from keras import optimizers\n",
    "\n",
    "# 词向量维度为300\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN模型\n",
    "def RNN(embeddings_matrix, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH, labels_types):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                        output_dim = EMBEDDING_DIM, # 词向量长度（300）\n",
    "                        weights = [embeddings_matrix], # 预训练的词向量系数\n",
    "                        input_length = MAX_SEQUENCE_LENGTH, # 每句话的最大长度（必须padding） \n",
    "                        trainable = False, # 是否在训练的过程中更新词向量\n",
    "                       ),\n",
    "             )\n",
    "\n",
    "    ## input_shape=(Batch_size, Time_step, Input_Sizes)\n",
    "    # 输入层\n",
    "    model.add(SimpleRNN(128,\n",
    "                        input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM),\n",
    "                        activation='tanh', # 激活函数\n",
    "                        return_sequences=True, # 返回全部time step的hidden state值\n",
    "                       )\n",
    "             )\n",
    "    model.add(Dropout(0.5)) # 减少过拟合\n",
    "    #model.add(Flatten())\n",
    "\n",
    "    # 隐藏层\n",
    "    model.add(Dense(64, \n",
    "                    input_shape=(MAX_SEQUENCE_LENGTH, 128), # 128维\n",
    "                    activation='relu',\n",
    "                   )\n",
    "             )\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # 输出层\n",
    "    model.add(Dense(len(labels_types)+1,\n",
    "                    input_shape=(MAX_SEQUENCE_LENGTH, 64),  # 64维\n",
    "                    activation='softmax',\n",
    "                   )\n",
    "             )\n",
    "    return model\n",
    "\n",
    "model = RNN(embeddings_matrix, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH, labels_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRNN模型\n",
    "def BRNN(embeddings_matrix, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH, labels_types):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                        output_dim = EMBEDDING_DIM, # 词向量长度（300）\n",
    "                        weights = [embeddings_matrix], # 预训练的词向量系数\n",
    "                        input_length = MAX_SEQUENCE_LENGTH, # 每句话的最大长度（必须padding） \n",
    "                        trainable = False, # 是否在训练的过程中更新词向量\n",
    "                       ),\n",
    "             )\n",
    "\n",
    "    ## input_shape=(Batch_size, Time_step, Input_Sizes)\n",
    "    # 输入层（添加bidirctional）\n",
    "    model.add(Bidirectional(SimpleRNN(128, \n",
    "                                      input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), \n",
    "                                      activation='tanh', \n",
    "                                      return_sequences=True\n",
    "                                     ), \n",
    "                             merge_mode='concat',\n",
    "                           ),\n",
    "             )\n",
    "    model.add(Dropout(0.5)) # 减少过拟合\n",
    "\n",
    "    # 隐藏层\n",
    "    model.add(Dense(64, \n",
    "                    input_shape=(MAX_SEQUENCE_LENGTH, 128), # 128维\n",
    "                    activation='relu',\n",
    "                   )\n",
    "             )\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # 输出层\n",
    "    model.add(Dense(len(labels_types)+1,\n",
    "                    input_shape=(MAX_SEQUENCE_LENGTH, 64),  # 64维\n",
    "                    activation='softmax',\n",
    "                   )\n",
    "             )\n",
    "    return model\n",
    "\n",
    "model = BRNN(embeddings_matrix, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH, labels_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 未进行标签平滑\n",
    "# 调节RMSprop的学习率参数lr为0.001\n",
    "rms = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', # 损失函数\n",
    "              optimizer=rms, # 优化器采用'rmsprop'\n",
    "              metrics=['accuracy'], # 评价指标是准确率\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 进行标签平滑\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# 标签平滑损失函数\n",
    "loss = CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "# 调节RMSprop的学习率参数lr为0.001\n",
    "rms = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(loss=loss, # 损失函数\n",
    "              optimizer=rms, # 优化器采用'rmsprop'\n",
    "              metrics=['accuracy'], # 评价指标是准确率\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          105665400 \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 256)          109824    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100, 64)           16448     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100, 37)           2405      \n",
      "=================================================================\n",
      "Total params: 105,794,077\n",
      "Trainable params: 128,677\n",
      "Non-trainable params: 105,665,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 输出模型各层的参数状况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中将词语转换为索引后: (18078, 100)\n",
      "\n",
      "训练集中将标签转换为索引并填充张量后 (18078, 100, 37)\n"
     ]
    }
   ],
   "source": [
    "# 输出训练集中将词语转换为索引后的shape\n",
    "print('训练集中将词语转换为索引后:', X_train_tokenized.shape)\n",
    "# 18078个样本,每个样本100个词\n",
    "\n",
    "# 输出训练集中将标签转换为索引并填充张量后的shape\n",
    "print('\\n训练集中将标签转换为索引并填充张量后', y_train_index_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 51s 357ms/step - loss: 0.5878 - accuracy: 0.8536\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 48s 337ms/step - loss: 0.2109 - accuracy: 0.9365\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 51s 361ms/step - loss: 0.1596 - accuracy: 0.9501\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 43s 302ms/step - loss: 0.1416 - accuracy: 0.9552\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 44s 307ms/step - loss: 0.1322 - accuracy: 0.9578\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 44s 309ms/step - loss: 0.1263 - accuracy: 0.9596\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 44s 312ms/step - loss: 0.1222 - accuracy: 0.9608\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 44s 312ms/step - loss: 0.1190 - accuracy: 0.9614\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 44s 307ms/step - loss: 0.1170 - accuracy: 0.9620\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 43s 306ms/step - loss: 0.1147 - accuracy: 0.9626\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 43s 305ms/step - loss: 0.1128 - accuracy: 0.9632\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 44s 307ms/step - loss: 0.1114 - accuracy: 0.9636\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 44s 309ms/step - loss: 0.1099 - accuracy: 0.9642\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 45s 314ms/step - loss: 0.1084 - accuracy: 0.9644\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 44s 307ms/step - loss: 0.1076 - accuracy: 0.9647\n",
      "Epoch 16/20\n",
      "142/142 [==============================] - 44s 307ms/step - loss: 0.1064 - accuracy: 0.9651\n",
      "Epoch 17/20\n",
      "142/142 [==============================] - 44s 307ms/step - loss: 0.1056 - accuracy: 0.9652\n",
      "Epoch 18/20\n",
      "142/142 [==============================] - 44s 309ms/step - loss: 0.1044 - accuracy: 0.9655\n",
      "Epoch 19/20\n",
      "142/142 [==============================] - 44s 309ms/step - loss: 0.1040 - accuracy: 0.9657\n",
      "Epoch 20/20\n",
      "142/142 [==============================] - 44s 306ms/step - loss: 0.1034 - accuracy: 0.9660\n",
      "-------------------Evaluation------------------------\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0530 - accuracy: 0.9813\n"
     ]
    }
   ],
   "source": [
    "# 使用训练集对模型进行训练\n",
    "model.fit(X_train_tokenized,\n",
    "          y_train_index_padded,\n",
    "          epochs = 20,  # 训练20轮\n",
    "          batch_size = 128, # 指定进行梯度下降时每个batch包含的样本数为128\n",
    "          verbose = 1, # 展示训练过程\n",
    "         )\n",
    "\n",
    "# 评估训练完成的模型.返回损失值&模型的度量值.\n",
    "print('-------------------Evaluation------------------------')\n",
    "score = model.evaluate(X_test_tokenized, \n",
    "                       y_test_index_padded,\n",
    "                       batch_size=128,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存rnn模型\n",
    "model.save('cn_pos_tag_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存brnn模型\n",
    "model.save('cn_pos_tag_brnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练的模型经在测试集上验证获得的loss和accuracy为：\n",
      "[0.05298652499914169, 0.9812643527984619]\n"
     ]
    }
   ],
   "source": [
    "print('训练的模型经在测试集上验证获得的loss和accuracy为：')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------对测试集进行预测------------------------\n",
      "（1）对测试集 X_test_tokenized[:1] 进行预测：\n",
      "[[[2.9467149e-12 1.3376215e-18 6.6286404e-05 ... 3.3294791e-03\n",
      "   1.7201482e-12 1.4566714e-13]\n",
      "  [8.4083426e-19 7.2030519e-25 8.7497187e-07 ... 1.5803838e-08\n",
      "   3.0710169e-23 8.6442037e-18]\n",
      "  [1.3564103e-21 1.0349561e-24 6.6359036e-08 ... 3.0411465e-12\n",
      "   1.7746174e-18 3.3569497e-17]\n",
      "  ...\n",
      "  [9.9999988e-01 2.0126665e-27 8.7387924e-08 ... 3.3107492e-14\n",
      "   1.3039966e-23 6.8032123e-18]\n",
      "  [9.9999964e-01 5.3980682e-25 3.1134417e-07 ... 6.8773719e-13\n",
      "   1.6942969e-21 2.3384099e-16]\n",
      "  [9.9999857e-01 1.4131538e-22 1.1144496e-06 ... 1.7796905e-11\n",
      "   3.4814879e-19 1.2530364e-14]]]\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行预测\n",
    "print('-----------------------对测试集进行预测------------------------')\n",
    "print('（1）对测试集 X_test_tokenized[:1] 进行预测：')\n",
    "print(model.predict(X_test_tokenized[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中标签填充的张量 y_test_index_padded[:1] 为：\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 输出测试集中标签填充的张量\n",
    "print('测试集中标签填充的张量 y_test_index_padded[:1] 为：')\n",
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（2）对测试集 X_test_tokenized[2: 3] 进行预测：\n",
      "[[[2.1755171e-14 5.7821930e-28 9.9999845e-01 ... 1.0647330e-14\n",
      "   5.0780611e-23 1.7765798e-08]\n",
      "  [1.5606285e-11 7.1746576e-16 8.5157044e-05 ... 5.7841246e-03\n",
      "   3.4609267e-11 4.5957384e-13]\n",
      "  [2.0214038e-18 1.9453082e-25 2.9080093e-06 ... 2.8483396e-09\n",
      "   1.4453245e-21 1.1426978e-18]\n",
      "  ...\n",
      "  [9.9999988e-01 2.0126665e-27 8.7387924e-08 ... 3.3107428e-14\n",
      "   1.3039966e-23 6.8032123e-18]\n",
      "  [9.9999964e-01 5.3981091e-25 3.1134476e-07 ... 6.8773719e-13\n",
      "   1.6943034e-21 2.3384099e-16]\n",
      "  [9.9999857e-01 1.4131538e-22 1.1144484e-06 ... 1.7796905e-11\n",
      "   3.4815014e-19 1.2530317e-14]]]\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行预测\n",
    "print('（2）对测试集 X_test_tokenized[2: 3] 进行预测：')\n",
    "print(model.predict(X_test_tokenized[2: 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中标签填充的张量 y_test_index_padded[:1] 为：\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 输出测试集中标签填充的张量\n",
    "print('测试集中标签填充的张量 y_test_index_padded[:1] 为：')\n",
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
